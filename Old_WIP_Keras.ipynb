{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "\n",
    "I'll describe the model's architecture, from beginning to end.\n",
    "\n",
    "What we're looking for is an image hashing function for retrieving similar images. The model should take in an image, and spit something out the other end. That thing, when interpreted as a point in n-dimensional space, should be close to other images like it, but far away from ones that aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a convolutional neural network, utilizing self-supervised transfer learning. I'll step through what this means.\n",
    "\n",
    "I've chosen my domain of choice to be digital illustrations. An algorithm called \"phash\"(1) has already proven to be incredibly effective at solving the near-duplicate image retrieval problem in hamming space for real pictures. However, the algorithm since it utilizes a discrete cosine transform, the same technique used in lossy jpg compression, suffers some of the same downsides. Since cosine waves are continous, they are not the greatest at finding the principal components images with hard lines, usually resulting in a noisier image on the other end. In the domain of real life pictures this is less of a problem. With bitmap images it is more so.\n",
    "\n",
    "We begin with one dataset full of digital illustrations, downloaded from the anime artwork website Safebooru. These are resized to 32x32, and stored to disk in a \"Train\" folder. I've already written a Java program that does this. Source code will be linked. As it is being trained on anime illustrations, the model may only be particulary useful at generating hashes of anime illustrations. But it can be retrained with the dataset of your choice. \n",
    "\n",
    "The full model will take in an image, and immediately the architecture has a branch. The original image is passed to multiple other layers which mess the image up in various ways. One downfall of phash, and in fact many perceptual image hashing algorithms is that they are not invariant to flips or rotation.(2) Whereas a human can look at two images and tell that they are duplicates if they are rotated, it is a much more difficult task in the realm of perceptual image hashing, unless you want to compute a lot more hashes on flipped and rotated images that take up a lot of extra time to compute and space to store, or otherwise use a hash that is not very robust to noise. With a machine learning approach, flips and rotations are much easier to account for. Just train on flipped and rotated images.\n",
    "\n",
    "Each of the scrombled images, plus the original, is fed into a convolutional neural network which compresses the image down smaller and smaller with convolutional and max pooling layers. At the end they are fed into a densely connected network, which produces what we're looking for, the latent space of the images. The hash that we're looking for. All that's left to do is to try and reconstruct the image and compute loss. \n",
    "\n",
    "So, I said earlier that the aim of the whole ordeal is to create a hash of the image which is close to similar images in n-dimensional space. It makes sense that we would train on that. The scrombled images are all similar. Therefore we want to have all of them cross-compare to each other, with as little loss as possible. \n",
    "\n",
    "But there's a problem. If that's all we train on, and all the network ever sees is at one time are images that are the same, eventually all the hashes may start coming out the same too. We are not incentivising perceptually distant images to have distant hash vectors. A potential solution is that I could just choose some number as a distance between hashes that aren't the same. But, who am I to choose that? Isn't machine learning here for the purpose of not having to make those decisions? Instead, let's train on another metric alongside n-dimensional distance. Let's train on reconstruction as well.\n",
    "\n",
    "We can pass the original image through that decoder that I mentioned, and also train on how well the image can be decoded. This way, the vector coming from the autodecoder must also be in some sense a faithful representation of the image. By making the decoder need to be able to \"know what the original looked like,\" we can make the encoder preserve that notion. \n",
    "\n",
    "Additionally, this method has actually been used to some success in the past for the purposes of retrieving similar images.(3) Just the act of compression in an autoencoder performing principle component analysis on the contents of an image makes images that are similar produce similar vectors. However, my approach varies from those that have been taken in the past.\n",
    "\n",
    "Earlier, I called my approach self-supervised transfer learning. But I haven't said what that means. Self-supervised is a term that came about because it isn't \"unsupervised\", at least not in the traditional sense of the literature, but we also aren't telling the model what the latent space should look like. We're imposing a set of criteria, but we aren't providing it examples or otherwise telling it how to do its job. Hence we are not supervising it. \n",
    "\n",
    "The approach utilizes transfer learning because I don't actually evaluate the whole thing all at once. First I train a really good autoencoder, and then I make modifications. The autoencoder distills the essence of the information, and then we tweak it so that it lines up. \n",
    "\n",
    "Finally, we can remove the decoder, all the brances, and the scrombling layer, we'll have a usable encoder for image hashing. Or we won't. Let's find out together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1) [phash.org](phash.org)\n",
    "\n",
    "2) [phash Thesis](https://www.phash.org/docs/pubs/thesis_zauner.pdf) (Provides a great overview of percepual image hashing)\n",
    "\n",
    "3) [Autoencoder Image Retrieval](https://github.com/ankonzoid/artificio/tree/master/image_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from PIL import Image\n",
    "from urllib.request import urlopen\n",
    "from io import BytesIO\n",
    "from IPython.core.display import display, HTML\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.layers import MaxPooling2D, UpSampling2D\n",
    "from keras.layers import Flatten, Reshape\n",
    "from keras.layers import GaussianNoise\n",
    "from keras.utils import plot_model\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: (32, 32, 3)\n",
      "latent shape: (None, 20)\n",
      "compression ratio: 0.65%\n",
      "convolutional layers: 2\n",
      "dense layers: 3\n",
      "\n",
      "loss weights (distance, mse): (1, 1)\n",
      "\n",
      "number of ed models to train: 5\n",
      "epochs: 500\n"
     ]
    }
   ],
   "source": [
    "side_length = 32\n",
    "i_shape = (side_length, side_length, 3)\n",
    "latent_shape = (None, 20)\n",
    "\n",
    "convlayers = 2\n",
    "denselayers = 3\n",
    "\n",
    "distance_weight = 1\n",
    "mse_weight = 1\n",
    "\n",
    "batch_size = 32\n",
    "epoch_num = 500\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)]\n",
    "train_ed = True\n",
    "num_models = 5\n",
    "\n",
    "print(f'input shape: {i_shape}')\n",
    "print(f'latent shape: {latent_shape}')\n",
    "print('compression ratio: {:.2%}'.format(latent_shape[1] / np.prod(i_shape))) \n",
    "print(f'convolutional layers: {convlayers}')\n",
    "print(f'dense layers: {denselayers}\\n')\n",
    "print(f'loss weights (distance, mse): ({distance_weight}, {mse_weight})\\n')\n",
    "print(f'number of ed models to train: {num_models if train_ed else 0}')\n",
    "print(f'epochs: {epoch_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO replace with Safebooru dataset once I have storage\n",
    "from keras.datasets import cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='coarse')\n",
    "training_samples   = x_train.shape[0]\n",
    "validation_samples =  x_test.shape[0]\n",
    "\n",
    "validation_ratio = .25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encode\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "Convolution1 (Conv2D)        (None, 32, 32, 3)         84        \n",
      "_________________________________________________________________\n",
      "MaxPool1 (MaxPooling2D)      (None, 16, 16, 3)         0         \n",
      "_________________________________________________________________\n",
      "Convolution2 (Conv2D)        (None, 16, 16, 3)         84        \n",
      "_________________________________________________________________\n",
      "MaxPool2 (MaxPooling2D)      (None, 8, 8, 3)           0         \n",
      "_________________________________________________________________\n",
      "Flatten (Flatten)            (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "Dense1 (Dense)               (None, 192)               37056     \n",
      "_________________________________________________________________\n",
      "Dense2 (Dense)               (None, 135)               26055     \n",
      "_________________________________________________________________\n",
      "Dense3 (Dense)               (None, 78)                10608     \n",
      "_________________________________________________________________\n",
      "Final_Dense (Dense)          (None, 20)                1580      \n",
      "=================================================================\n",
      "Total params: 75,467\n",
      "Trainable params: 75,467\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_encoder(i_shape, latent_shape, convlayers, denselayers):\n",
    "    img_input = Input(shape=i_shape)\n",
    "    img = img_input\n",
    "    \n",
    "    # Define convolution/pooling layers\n",
    "    conv_kernel_shape = (3, 3)\n",
    "    max_pool_shape = (2, 2)\n",
    "    convs = [ Conv2D(3, conv_kernel_shape, activation='relu', padding='same', name=f'Convolution{i+1}') for i in range(0, convlayers) ] \n",
    "    pools = [ MaxPooling2D(max_pool_shape, padding='same', name=f'MaxPool{i+1}') for i in range(0, convlayers) ] \n",
    "    \n",
    "    # Convolve and pool image, then flatten\n",
    "    for i in range(0, convlayers):\n",
    "        img = pools[i](convs[i](img))\n",
    "    img = Flatten(name='Flatten')(img)\n",
    "\n",
    "    # Define dense layers and apply\n",
    "    inprod = (np.prod(i_shape) // (4**(convlayers)))\n",
    "    squeeze_step = (inprod - latent_shape[1]) // denselayers\n",
    "    \n",
    "    hiddens = [ Dense(inprod - i*squeeze_step, activation='relu', name=f'Dense{i+1}') for i in range(0, denselayers) ]\n",
    "    for i in range(0, denselayers):\n",
    "        img = hiddens[i](img)\n",
    "    img = Dense(latent_shape[1], activation='relu', name='Final_Dense')(img)\n",
    "    \n",
    "    encoder = Model(inputs=img_input, outputs=img, name='Encode')\n",
    "    return encoder\n",
    "print(build_encoder(i_shape, latent_shape, convlayers, denselayers).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Decode\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, None, 20)          0         \n",
      "_________________________________________________________________\n",
      "Inv_Convolution3 (Dense)     (None, None, 78)          1638      \n",
      "_________________________________________________________________\n",
      "Inv_Convolution2 (Dense)     (None, None, 135)         10665     \n",
      "_________________________________________________________________\n",
      "Inv_Convolution1 (Dense)     (None, None, 192)         26112     \n",
      "_________________________________________________________________\n",
      "Reshape (Reshape)            (None, 8, 8, 3)           0         \n",
      "_________________________________________________________________\n",
      "Upsample1 (UpSampling2D)     (None, 16, 16, 3)         0         \n",
      "_________________________________________________________________\n",
      "unconvolve1 (Conv2DTranspose (None, 16, 16, 3)         84        \n",
      "_________________________________________________________________\n",
      "Upsample2 (UpSampling2D)     (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "unconvolve2 (Conv2DTranspose (None, 32, 32, 3)         84        \n",
      "=================================================================\n",
      "Total params: 38,583\n",
      "Trainable params: 38,583\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def build_decoder(i_shape, latent_shape, convlayers, denselayers):\n",
    "    encoded_input = Input(latent_shape)\n",
    "    img = encoded_input\n",
    "    \n",
    "    inprod = (np.prod(i_shape) // (4**(convlayers)))\n",
    "    squeeze_step = (inprod - latent_shape[1]) // denselayers\n",
    "    inv_hiddens = [ Dense(inprod - i*squeeze_step, activation='relu', name=f'Inv_Convolution{i+1}') for i in range(0, denselayers) ] \n",
    "    inv_hiddens = inv_hiddens[::-1]\n",
    "    for i in range(0, denselayers):\n",
    "        img = inv_hiddens[i](img)\n",
    "        \n",
    "    # Unflatten\n",
    "    convolved_sidelength = i_shape[1] // (2**convlayers)\n",
    "    img = Reshape((convolved_sidelength, convolved_sidelength, 3), name='Reshape')(img)\n",
    "\n",
    "    # Upscale and unconvolve\n",
    "    ups = [ UpSampling2D((2, 2), name=f'Upsample{i+1}') for i in range(0, convlayers) ]\n",
    "    unconvs = [ Conv2DTranspose(3, (3, 3), activation='relu', padding='same', name=f'unconvolve{i+1}') for i in range(0, convlayers) ] \n",
    "    for i in range(0, convlayers):\n",
    "            img = unconvs[i](ups[i](img))\n",
    "\n",
    "    decoder = Model(inputs=encoded_input, outputs=img, name='Decode')\n",
    "    return decoder\n",
    "print(build_decoder(i_shape, latent_shape, convlayers, denselayers).summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# History Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.subplot(212)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode/Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encode_decode(i_shape, latent_shape, convlayers, denselayers):\n",
    "    encode_decode_in = Input(i_shape)\n",
    "    encode_decode_out = build_decoder(i_shape, latent_shape, convlayers, denselayers)(build_encoder(i_shape, latent_shape, convlayers, denselayers)(encode_decode_in))\n",
    "    encode_decode = Model(inputs=encode_decode_in, outputs=encode_decode_out, name='Encode_Decode')\n",
    "    #print(encode_decode.summary())\n",
    "    return encode_decode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Encoder and Decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 2624.1555 - val_loss: 1798.7113\n",
      "Epoch 2/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1712.0095 - val_loss: 1697.9119\n",
      "Epoch 3/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1650.1567 - val_loss: 1668.3898\n",
      "Epoch 4/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1628.7748 - val_loss: 1668.6570\n",
      "Epoch 5/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1606.0546 - val_loss: 1613.3470\n",
      "Epoch 6/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1578.4385 - val_loss: 1603.7742\n",
      "Epoch 7/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1570.2165 - val_loss: 1591.5064\n",
      "Epoch 8/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1564.3728 - val_loss: 1597.6464\n",
      "Epoch 9/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1559.9946 - val_loss: 1594.2645\n",
      "Epoch 10/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1556.5119 - val_loss: 1580.9005\n",
      "Epoch 11/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1554.1601 - val_loss: 1580.6436\n",
      "Epoch 12/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1550.1286 - val_loss: 1573.0541\n",
      "Epoch 13/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1548.0621 - val_loss: 1572.8707\n",
      "Epoch 14/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1545.6343 - val_loss: 1572.1252\n",
      "Epoch 15/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1543.3030 - val_loss: 1576.9766\n",
      "Epoch 16/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1541.8122 - val_loss: 1569.7348\n",
      "Epoch 17/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1539.8456 - val_loss: 1568.1915\n",
      "Epoch 18/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1537.7555 - val_loss: 1571.4074\n",
      "Epoch 19/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1536.7082 - val_loss: 1566.4247\n",
      "Epoch 20/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1534.6668 - val_loss: 1565.8463\n",
      "Epoch 21/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1533.8193 - val_loss: 1578.1199\n",
      "Epoch 22/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1532.1462 - val_loss: 1558.5385\n",
      "Epoch 23/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1531.0284 - val_loss: 1558.9283\n",
      "Epoch 24/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1530.1373 - val_loss: 1560.1742\n",
      "Epoch 25/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1528.7342 - val_loss: 1558.9794\n",
      "Epoch 26/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1528.0614 - val_loss: 1567.9273\n",
      "Epoch 27/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1526.8330 - val_loss: 1556.4521\n",
      "Epoch 28/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1526.0284 - val_loss: 1560.5832\n",
      "Epoch 29/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1525.3517 - val_loss: 1563.2234\n",
      "Epoch 30/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1523.9974 - val_loss: 1551.7759\n",
      "Epoch 31/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1523.4865 - val_loss: 1548.3280\n",
      "Epoch 32/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1522.3685 - val_loss: 1552.4679\n",
      "Epoch 33/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1521.2451 - val_loss: 1548.8133\n",
      "Epoch 34/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1520.3504 - val_loss: 1546.5095\n",
      "Epoch 35/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1519.3851 - val_loss: 1552.6820\n",
      "Epoch 36/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1518.0977 - val_loss: 1553.2740\n",
      "Epoch 37/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1517.7734 - val_loss: 1559.5827\n",
      "Epoch 38/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1517.0505 - val_loss: 1548.1157\n",
      "Epoch 39/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1515.7748 - val_loss: 1553.4780\n",
      "Epoch 40/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1515.4044 - val_loss: 1544.4156\n",
      "Epoch 41/500\n",
      "50000/50000 [==============================] - 74s 1ms/step - loss: 1514.2642 - val_loss: 1542.4671\n",
      "Epoch 42/500\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 1513.9207 - val_loss: 1550.7574\n",
      "Epoch 43/500\n",
      "50000/50000 [==============================] - 97s 2ms/step - loss: 1513.4608 - val_loss: 1548.9835\n",
      "Epoch 44/500\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 1512.4083 - val_loss: 1542.4800\n",
      "Epoch 45/500\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 1512.0382 - val_loss: 1549.0264\n",
      "Epoch 46/500\n",
      "50000/50000 [==============================] - 108s 2ms/step - loss: 1510.5552 - val_loss: 1541.4902\n",
      "Epoch 47/500\n",
      "50000/50000 [==============================] - 108s 2ms/step - loss: 1510.9193 - val_loss: 1541.9375\n",
      "Epoch 48/500\n",
      "50000/50000 [==============================] - 102s 2ms/step - loss: 1509.8149 - val_loss: 1539.4025\n",
      "Epoch 49/500\n",
      "50000/50000 [==============================] - 113s 2ms/step - loss: 1509.0358 - val_loss: 1535.9259\n",
      "Epoch 50/500\n",
      "50000/50000 [==============================] - 104s 2ms/step - loss: 1508.5843 - val_loss: 1538.5425\n",
      "Epoch 51/500\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1507.0494 - val_loss: 1546.3406\n",
      "Epoch 52/500\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 1506.8325 - val_loss: 1554.2995\n",
      "Epoch 53/500\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 1507.2097 - val_loss: 1550.9202\n",
      "Epoch 54/500\n",
      "50000/50000 [==============================] - 103s 2ms/step - loss: 1506.0296 - val_loss: 1546.5154\n",
      "Epoch 55/500\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1505.2122 - val_loss: 1542.0696\n",
      "Epoch 56/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1505.6722 - val_loss: 1544.3595\n",
      "Epoch 57/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1504.6360 - val_loss: 1535.1657\n",
      "Epoch 58/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1503.7554 - val_loss: 1536.1499\n",
      "Epoch 59/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1503.5318 - val_loss: 1537.7900\n",
      "Epoch 60/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1503.4852 - val_loss: 1536.4933\n",
      "Epoch 61/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1502.8297 - val_loss: 1538.9231\n",
      "Epoch 62/500\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1501.9857 - val_loss: 1531.9925\n",
      "Epoch 63/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1502.3686 - val_loss: 1532.6006\n",
      "Epoch 64/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1501.3481 - val_loss: 1536.8025\n",
      "Epoch 65/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1501.0025 - val_loss: 1531.1674\n",
      "Epoch 66/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1500.5760 - val_loss: 1532.9488\n",
      "Epoch 67/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1500.6895 - val_loss: 1535.3133\n",
      "Epoch 68/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1500.6453 - val_loss: 1534.3602\n",
      "Epoch 69/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1499.7550 - val_loss: 1532.5538\n",
      "Epoch 70/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1499.6960 - val_loss: 1528.9340\n",
      "Epoch 71/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1499.2890 - val_loss: 1534.3262\n",
      "Epoch 72/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1498.3954 - val_loss: 1532.3549\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1498.6386 - val_loss: 1530.5238\n",
      "Epoch 74/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1498.1995 - val_loss: 1536.0858\n",
      "Epoch 75/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1497.9762 - val_loss: 1531.3047\n",
      "Epoch 76/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1497.2306 - val_loss: 1537.2636\n",
      "Epoch 77/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1497.6605 - val_loss: 1535.8666\n",
      "Epoch 78/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1496.8419 - val_loss: 1532.3768\n",
      "Epoch 79/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1496.8825 - val_loss: 1536.6198\n",
      "Epoch 80/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1496.2977 - val_loss: 1529.5728\n",
      "Epoch 81/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1495.8673 - val_loss: 1530.5367\n",
      "Epoch 82/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1496.1131 - val_loss: 1534.1315\n",
      "Epoch 83/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1495.8497 - val_loss: 1529.9224\n",
      "Epoch 84/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1495.7371 - val_loss: 1541.5981\n",
      "Epoch 85/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1495.3062 - val_loss: 1535.1500\n",
      "Saving model: models/encode-decode/1528_32_20_2_3_1_1_5003395554.h5\n",
      "Best Epoch: 69\n",
      "Current Loss: 1528.9340373046875\n",
      "Best Loss: 1528.9340373046875\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 2543.7787 - val_loss: 1703.3286\n",
      "Epoch 2/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1616.5141 - val_loss: 1602.5838\n",
      "Epoch 3/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1554.7334 - val_loss: 1562.9775\n",
      "Epoch 4/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1536.0058 - val_loss: 1553.9647\n",
      "Epoch 5/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1517.0772 - val_loss: 1519.7928\n",
      "Epoch 6/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1500.0224 - val_loss: 1530.1517\n",
      "Epoch 7/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1493.3315 - val_loss: 1544.6282\n",
      "Epoch 8/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1488.2920 - val_loss: 1522.7971\n",
      "Epoch 9/500\n",
      "50000/50000 [==============================] - 27417s 548ms/step - loss: 1484.7941 - val_loss: 1507.7207\n",
      "Epoch 10/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1481.6076 - val_loss: 1513.6008\n",
      "Epoch 11/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1479.6504 - val_loss: 1507.0344\n",
      "Epoch 12/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1477.2838 - val_loss: 1502.9586\n",
      "Epoch 13/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1474.3633 - val_loss: 1499.1869\n",
      "Epoch 14/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1472.7616 - val_loss: 1499.5239\n",
      "Epoch 15/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1472.3093 - val_loss: 1497.5826\n",
      "Epoch 16/500\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 1469.3868 - val_loss: 1492.7031\n",
      "Epoch 17/500\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 1467.6865 - val_loss: 1494.1812\n",
      "Epoch 18/500\n",
      "50000/50000 [==============================] - 85s 2ms/step - loss: 1466.1768 - val_loss: 1491.1149\n",
      "Epoch 19/500\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 1464.3263 - val_loss: 1491.4863\n",
      "Epoch 20/500\n",
      "50000/50000 [==============================] - 85s 2ms/step - loss: 1464.0624 - val_loss: 1505.8903\n",
      "Epoch 21/500\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1461.8957 - val_loss: 1494.6249\n",
      "Epoch 22/500\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 1461.0058 - val_loss: 1487.3249\n",
      "Epoch 23/500\n",
      "50000/50000 [==============================] - 115s 2ms/step - loss: 1459.8191 - val_loss: 1505.5929\n",
      "Epoch 24/500\n",
      "50000/50000 [==============================] - 117s 2ms/step - loss: 1459.5296 - val_loss: 1489.3618\n",
      "Epoch 25/500\n",
      "50000/50000 [==============================] - 100s 2ms/step - loss: 1457.7520 - val_loss: 1483.3798\n",
      "Epoch 26/500\n",
      "50000/50000 [==============================] - 106s 2ms/step - loss: 1457.4866 - val_loss: 1485.1591\n",
      "Epoch 27/500\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 1455.9361 - val_loss: 1486.7483\n",
      "Epoch 28/500\n",
      "50000/50000 [==============================] - 108s 2ms/step - loss: 1455.0081 - val_loss: 1482.1721\n",
      "Epoch 29/500\n",
      "50000/50000 [==============================] - 90s 2ms/step - loss: 1454.2581 - val_loss: 1480.2723\n",
      "Epoch 30/500\n",
      "50000/50000 [==============================] - 84s 2ms/step - loss: 1454.1320 - val_loss: 1480.7902\n",
      "Epoch 31/500\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 1453.1543 - val_loss: 1485.0783\n",
      "Epoch 32/500\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 1452.0669 - val_loss: 1484.3043\n",
      "Epoch 33/500\n",
      "50000/50000 [==============================] - 69s 1ms/step - loss: 1451.6923 - val_loss: 1480.0270\n",
      "Epoch 34/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1451.0352 - val_loss: 1484.7616\n",
      "Epoch 35/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1450.3759 - val_loss: 1476.6741\n",
      "Epoch 36/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1449.6735 - val_loss: 1490.0714\n",
      "Epoch 37/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1449.1728 - val_loss: 1477.2034\n",
      "Epoch 38/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1448.8693 - val_loss: 1479.8568\n",
      "Epoch 39/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1448.2974 - val_loss: 1474.6523\n",
      "Epoch 40/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1447.4179 - val_loss: 1477.7586\n",
      "Epoch 41/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1446.1591 - val_loss: 1493.4471\n",
      "Epoch 42/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1445.7410 - val_loss: 1471.3933\n",
      "Epoch 43/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1444.7636 - val_loss: 1475.6058\n",
      "Epoch 44/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1444.5527 - val_loss: 1475.9893\n",
      "Epoch 45/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1444.2404 - val_loss: 1474.2291\n",
      "Epoch 46/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1443.7994 - val_loss: 1492.4672\n",
      "Epoch 47/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1443.2463 - val_loss: 1474.5473\n",
      "Epoch 48/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1442.6858 - val_loss: 1474.1901\n",
      "Epoch 49/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1442.0369 - val_loss: 1475.1049\n",
      "Epoch 50/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1442.2172 - val_loss: 1472.9417\n",
      "Epoch 51/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1442.1292 - val_loss: 1469.4485\n",
      "Epoch 52/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1441.7412 - val_loss: 1468.5061\n",
      "Epoch 53/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1440.3465 - val_loss: 1476.3945\n",
      "Epoch 54/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1440.2039 - val_loss: 1474.0302\n",
      "Epoch 55/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1440.2707 - val_loss: 1465.0214\n",
      "Epoch 56/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1439.5588 - val_loss: 1465.8222\n",
      "Epoch 57/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1439.0646 - val_loss: 1473.1309\n",
      "Epoch 58/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1438.7633 - val_loss: 1471.4993\n",
      "Epoch 59/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1438.7590 - val_loss: 1467.1882\n",
      "Epoch 60/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1437.9936 - val_loss: 1468.0748\n",
      "Epoch 61/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1438.4006 - val_loss: 1472.0006\n",
      "Epoch 62/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1437.2457 - val_loss: 1468.7233\n",
      "Epoch 63/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1437.4100 - val_loss: 1464.7041\n",
      "Epoch 64/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1437.1737 - val_loss: 1470.1828\n",
      "Epoch 65/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1436.7998 - val_loss: 1470.2454\n",
      "Epoch 66/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1436.6213 - val_loss: 1472.3569\n",
      "Epoch 67/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1436.2795 - val_loss: 1468.3261\n",
      "Epoch 68/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1435.5860 - val_loss: 1472.0229\n",
      "Epoch 69/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1436.1468 - val_loss: 1468.2657\n",
      "Epoch 70/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1435.8047 - val_loss: 1467.4844\n",
      "Epoch 71/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1435.4809 - val_loss: 1465.3801\n",
      "Epoch 72/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1434.8173 - val_loss: 1463.7878\n",
      "Epoch 73/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1434.3500 - val_loss: 1463.8702\n",
      "Epoch 74/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1434.2893 - val_loss: 1464.4593\n",
      "Epoch 75/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1434.4022 - val_loss: 1465.8832\n",
      "Epoch 76/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1433.4642 - val_loss: 1466.2589\n",
      "Epoch 77/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1434.2025 - val_loss: 1464.2048\n",
      "Epoch 78/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1433.5947 - val_loss: 1471.5601\n",
      "Epoch 79/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1433.3741 - val_loss: 1467.1820\n",
      "Epoch 80/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1432.4154 - val_loss: 1463.9778\n",
      "Epoch 81/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1433.2460 - val_loss: 1464.0455\n",
      "Epoch 82/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1432.8254 - val_loss: 1464.7408\n",
      "Epoch 83/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1432.4454 - val_loss: 1466.3865\n",
      "Epoch 84/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1431.8405 - val_loss: 1464.1981\n",
      "Epoch 85/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1431.9574 - val_loss: 1467.5824\n",
      "Epoch 86/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1431.7323 - val_loss: 1466.4063\n",
      "Epoch 87/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1431.6557 - val_loss: 1460.3288\n",
      "Epoch 88/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1431.4938 - val_loss: 1461.1877\n",
      "Epoch 89/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1431.6380 - val_loss: 1470.3488\n",
      "Epoch 90/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1430.8499 - val_loss: 1459.1459\n",
      "Epoch 91/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1431.2029 - val_loss: 1464.3408\n",
      "Epoch 92/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1430.5675 - val_loss: 1460.5914\n",
      "Epoch 93/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1430.7872 - val_loss: 1466.3986\n",
      "Epoch 94/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1430.1933 - val_loss: 1458.0219\n",
      "Epoch 95/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1429.7689 - val_loss: 1463.1362\n",
      "Epoch 96/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1430.2588 - val_loss: 1467.4369\n",
      "Epoch 97/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1430.1483 - val_loss: 1462.0415\n",
      "Epoch 98/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1429.9207 - val_loss: 1466.3360\n",
      "Epoch 99/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1429.1697 - val_loss: 1461.8329\n",
      "Epoch 100/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1429.4754 - val_loss: 1471.5068\n",
      "Epoch 101/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1428.6225 - val_loss: 1462.5489\n",
      "Epoch 102/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1429.6221 - val_loss: 1460.9104\n",
      "Epoch 103/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1428.4932 - val_loss: 1462.2779\n",
      "Epoch 104/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1429.7540 - val_loss: 1469.8154\n",
      "Epoch 105/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1428.0755 - val_loss: 1467.0147\n",
      "Epoch 106/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1428.0806 - val_loss: 1462.8478\n",
      "Epoch 107/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1428.3096 - val_loss: 1475.5758\n",
      "Epoch 108/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1427.9060 - val_loss: 1465.9787\n",
      "Epoch 109/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1427.9605 - val_loss: 1458.6172\n",
      "Saving model: models/encode-decode/1458_32_20_2_3_1_1_8769694034.h5\n",
      "Best Epoch: 93\n",
      "Current Loss: 1458.02185390625\n",
      "Best Loss: 1458.02185390625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 2651.1596 - val_loss: 1792.9340\n",
      "Epoch 2/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1698.3587 - val_loss: 1688.4410\n",
      "Epoch 3/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1629.8179 - val_loss: 1629.1644\n",
      "Epoch 4/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1584.2966 - val_loss: 1602.3378\n",
      "Epoch 5/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1568.9109 - val_loss: 1594.8746\n",
      "Epoch 6/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1558.4470 - val_loss: 1573.4672\n",
      "Epoch 7/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1538.7204 - val_loss: 1561.7829\n",
      "Epoch 8/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1530.7445 - val_loss: 1551.4638\n",
      "Epoch 9/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1526.4505 - val_loss: 1560.0346\n",
      "Epoch 10/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1522.4558 - val_loss: 1545.0649\n",
      "Epoch 11/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1518.1023 - val_loss: 1565.0765\n",
      "Epoch 12/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1515.3418 - val_loss: 1548.0150\n",
      "Epoch 13/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1513.6945 - val_loss: 1534.9481\n",
      "Epoch 14/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1510.7937 - val_loss: 1556.0087\n",
      "Epoch 15/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1509.6843 - val_loss: 1558.3867\n",
      "Epoch 16/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1501.2716 - val_loss: 1542.4090\n",
      "Epoch 17/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1498.1962 - val_loss: 1525.0723\n",
      "Epoch 18/500\n",
      "50000/50000 [==============================] - 54s 1ms/step - loss: 1495.5045 - val_loss: 1521.1559\n",
      "Epoch 19/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1493.2994 - val_loss: 1520.2335\n",
      "Epoch 20/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1492.2213 - val_loss: 1525.2450\n",
      "Epoch 21/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1490.7358 - val_loss: 1513.7848\n",
      "Epoch 22/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1489.4250 - val_loss: 1513.0164\n",
      "Epoch 23/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1476.9637 - val_loss: 1501.5711\n",
      "Epoch 24/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1470.2142 - val_loss: 1503.0213\n",
      "Epoch 25/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1468.2931 - val_loss: 1490.5894\n",
      "Epoch 26/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1466.3093 - val_loss: 1495.9211\n",
      "Epoch 27/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1465.9024 - val_loss: 1491.4343\n",
      "Epoch 28/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1463.6864 - val_loss: 1488.9824\n",
      "Epoch 29/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1463.3660 - val_loss: 1491.6841\n",
      "Epoch 30/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1462.1405 - val_loss: 1486.5889\n",
      "Epoch 31/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1460.4588 - val_loss: 1487.4519\n",
      "Epoch 32/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1460.5037 - val_loss: 1486.2525\n",
      "Epoch 33/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1459.7435 - val_loss: 1490.7389\n",
      "Epoch 34/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1458.4349 - val_loss: 1481.3535\n",
      "Epoch 35/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1457.3530 - val_loss: 1488.1809\n",
      "Epoch 36/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1457.3719 - val_loss: 1480.6022\n",
      "Epoch 37/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1456.9043 - val_loss: 1480.6262\n",
      "Epoch 38/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1456.1235 - val_loss: 1484.0453\n",
      "Epoch 39/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1455.5878 - val_loss: 1479.6392\n",
      "Epoch 40/500\n",
      "50000/50000 [==============================] - 53s 1ms/step - loss: 1454.4165 - val_loss: 1482.4839\n",
      "Epoch 41/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1454.6191 - val_loss: 1519.8875\n",
      "Epoch 42/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1453.2451 - val_loss: 1479.0599\n",
      "Epoch 43/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1454.0122 - val_loss: 1538.4459\n",
      "Epoch 44/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1452.8428 - val_loss: 1484.5405\n",
      "Epoch 45/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1452.2579 - val_loss: 1477.2783\n",
      "Epoch 46/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1451.6210 - val_loss: 1475.6209\n",
      "Epoch 47/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1451.5767 - val_loss: 1477.6928\n",
      "Epoch 48/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1452.0890 - val_loss: 1476.3022\n",
      "Epoch 49/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1450.6662 - val_loss: 1477.0725\n",
      "Epoch 50/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1451.1333 - val_loss: 1479.3849\n",
      "Epoch 51/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1450.3983 - val_loss: 1481.8080\n",
      "Epoch 52/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1450.0913 - val_loss: 1475.9087\n",
      "Epoch 53/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1449.2929 - val_loss: 1473.3173\n",
      "Epoch 54/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1449.2726 - val_loss: 1477.9370\n",
      "Epoch 55/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1449.2230 - val_loss: 1474.3528\n",
      "Epoch 56/500\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 1449.6507 - val_loss: 1475.0852\n",
      "Epoch 57/500\n",
      "50000/50000 [==============================] - 95s 2ms/step - loss: 1448.6044 - val_loss: 1482.7956\n",
      "Epoch 58/500\n",
      "50000/50000 [==============================] - 104s 2ms/step - loss: 1447.1579 - val_loss: 1474.6866\n",
      "Epoch 59/500\n",
      "50000/50000 [==============================] - 103s 2ms/step - loss: 1447.3922 - val_loss: 1486.7237\n",
      "Epoch 60/500\n",
      "50000/50000 [==============================] - 101s 2ms/step - loss: 1445.6834 - val_loss: 1480.9704\n",
      "Epoch 61/500\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 1446.5657 - val_loss: 1477.5323\n",
      "Epoch 62/500\n",
      "50000/50000 [==============================] - 86s 2ms/step - loss: 1446.3243 - val_loss: 1482.5426\n",
      "Epoch 63/500\n",
      "50000/50000 [==============================] - 104s 2ms/step - loss: 1447.4436 - val_loss: 1472.5014\n",
      "Epoch 64/500\n",
      "50000/50000 [==============================] - 97s 2ms/step - loss: 1444.7542 - val_loss: 1473.9398\n",
      "Epoch 65/500\n",
      "50000/50000 [==============================] - 100s 2ms/step - loss: 1445.1740 - val_loss: 1469.8375\n",
      "Epoch 66/500\n",
      "50000/50000 [==============================] - 101s 2ms/step - loss: 1445.2264 - val_loss: 1480.6990\n",
      "Epoch 67/500\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 1445.6595 - val_loss: 1490.4136\n",
      "Epoch 68/500\n",
      "50000/50000 [==============================] - 96s 2ms/step - loss: 1443.8698 - val_loss: 1473.0084\n",
      "Epoch 69/500\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1445.9574 - val_loss: 1470.0662\n",
      "Epoch 70/500\n",
      "50000/50000 [==============================] - 98s 2ms/step - loss: 1443.4032 - val_loss: 1475.6508\n",
      "Epoch 71/500\n",
      "50000/50000 [==============================] - 104s 2ms/step - loss: 1443.9156 - val_loss: 1469.7503\n",
      "Epoch 72/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 104s 2ms/step - loss: 1444.1621 - val_loss: 1473.5594\n",
      "Epoch 73/500\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 1442.7796 - val_loss: 1470.0485\n",
      "Epoch 74/500\n",
      "50000/50000 [==============================] - 105s 2ms/step - loss: 1443.4508 - val_loss: 1473.6435\n",
      "Epoch 75/500\n",
      "50000/50000 [==============================] - 107s 2ms/step - loss: 1442.3464 - val_loss: 1471.3338\n",
      "Epoch 76/500\n",
      "50000/50000 [==============================] - 109s 2ms/step - loss: 1443.2020 - val_loss: 1470.8480\n",
      "Epoch 77/500\n",
      "50000/50000 [==============================] - 88s 2ms/step - loss: 1443.4034 - val_loss: 1472.5834\n",
      "Epoch 78/500\n",
      "50000/50000 [==============================] - 74s 1ms/step - loss: 1443.6818 - val_loss: 1474.2086\n",
      "Epoch 79/500\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1441.7617 - val_loss: 1468.9962\n",
      "Epoch 80/500\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 1440.7951 - val_loss: 1475.1530\n",
      "Epoch 81/500\n",
      "50000/50000 [==============================] - 74s 1ms/step - loss: 1443.7036 - val_loss: 1469.1876\n",
      "Epoch 82/500\n",
      "50000/50000 [==============================] - 74s 1ms/step - loss: 1441.0595 - val_loss: 1467.2278\n",
      "Epoch 83/500\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 1441.1940 - val_loss: 1469.7912\n",
      "Epoch 84/500\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 1441.5258 - val_loss: 1491.5905\n",
      "Epoch 85/500\n",
      "50000/50000 [==============================] - 80s 2ms/step - loss: 1440.4710 - val_loss: 1476.4167\n",
      "Epoch 86/500\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 1441.1187 - val_loss: 1467.5604\n",
      "Epoch 87/500\n",
      "50000/50000 [==============================] - 78s 2ms/step - loss: 1442.7026 - val_loss: 1471.7716\n",
      "Epoch 88/500\n",
      "50000/50000 [==============================] - 77s 2ms/step - loss: 1439.5424 - val_loss: 1473.6336\n",
      "Epoch 89/500\n",
      "50000/50000 [==============================] - 79s 2ms/step - loss: 1443.2782 - val_loss: 1467.7174\n",
      "Epoch 90/500\n",
      "50000/50000 [==============================] - 92s 2ms/step - loss: 1441.0192 - val_loss: 1466.6191\n",
      "Epoch 91/500\n",
      "50000/50000 [==============================] - 81s 2ms/step - loss: 1440.3276 - val_loss: 1472.5430\n",
      "Epoch 92/500\n",
      "50000/50000 [==============================] - 72s 1ms/step - loss: 1439.1089 - val_loss: 1482.8563\n",
      "Epoch 93/500\n",
      "50000/50000 [==============================] - 73s 1ms/step - loss: 1440.2178 - val_loss: 1465.4571\n",
      "Epoch 94/500\n",
      "50000/50000 [==============================] - 75s 2ms/step - loss: 1441.5976 - val_loss: 1473.2853\n",
      "Epoch 95/500\n",
      "50000/50000 [==============================] - 72s 1ms/step - loss: 1439.3274 - val_loss: 1508.3621\n",
      "Epoch 96/500\n",
      "50000/50000 [==============================] - 73s 1ms/step - loss: 1439.9345 - val_loss: 1470.9554\n",
      "Epoch 97/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1438.4619 - val_loss: 1468.9080\n",
      "Epoch 98/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1439.1790 - val_loss: 1476.5006\n",
      "Epoch 99/500\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1438.8326 - val_loss: 1463.2647\n",
      "Epoch 109/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1436.7105 - val_loss: 1462.0073\n",
      "Epoch 110/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1437.4326 - val_loss: 1470.5706\n",
      "Epoch 111/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1436.6190 - val_loss: 1467.2496\n",
      "Epoch 112/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1437.3973 - val_loss: 1462.2204\n",
      "Epoch 113/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1438.9421 - val_loss: 1468.0158\n",
      "Epoch 114/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1436.1538 - val_loss: 1463.9665\n",
      "Epoch 115/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1435.5163 - val_loss: 1468.6111\n",
      "Epoch 116/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1435.7566 - val_loss: 1466.4634\n",
      "Epoch 117/500\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1436.3628 - val_loss: 1469.69471436.26\n",
      "Epoch 118/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1437.0537 - val_loss: 1466.2521\n",
      "Epoch 119/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1437.0333 - val_loss: 1463.4348\n",
      "Epoch 120/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1436.8393 - val_loss: 1463.3518\n",
      "Epoch 121/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1435.7891 - val_loss: 1466.0120\n",
      "Epoch 122/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1434.2862 - val_loss: 1484.3216\n",
      "Epoch 123/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1433.5227 - val_loss: 1467.2224\n",
      "Epoch 124/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1434.4346 - val_loss: 1458.6771\n",
      "Epoch 125/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1433.7910 - val_loss: 1463.6899\n",
      "Epoch 126/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1434.0281 - val_loss: 1460.7114\n",
      "Epoch 127/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1433.5030 - val_loss: 1459.2774\n",
      "Epoch 128/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1433.1733 - val_loss: 1462.9237\n",
      "Epoch 129/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1433.1185 - val_loss: 1461.8020\n",
      "Epoch 130/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1433.9701 - val_loss: 1463.9039\n",
      "Epoch 131/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1432.4891 - val_loss: 1465.3318\n",
      "Epoch 132/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1430.9716 - val_loss: 1466.8931\n",
      "Epoch 133/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1430.7494 - val_loss: 1463.3807\n",
      "Epoch 134/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1432.3253 - val_loss: 1463.9309\n",
      "Epoch 135/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1431.2703 - val_loss: 1464.8312\n",
      "Epoch 136/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1430.9435 - val_loss: 1459.9817\n",
      "Epoch 137/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1431.0387 - val_loss: 1461.4595\n",
      "Epoch 138/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1431.5636 - val_loss: 1461.2017\n",
      "Epoch 139/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1433.3744 - val_loss: 1464.2260\n",
      "Best Epoch: 123\n",
      "Current Loss: 1458.677098046875\n",
      "Best Loss: 1458.02185390625\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAACgCAYAAAD9/EDKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZhU9ZXw8e+ptfeG7qahpZFuCUYUFQSNqOOYxQU1asaMmsTEJE5w5s2i80SjJDGJmdd3Mu8kZptootFoEqNjXKKJmCCKJo4iNoiCoAKK0KxNA713rWf++N3qLppqqlmKqqbP53nqqVu/u9Sp7Z77W+peUVWMMcaYvfHlOwBjjDGFz5KFMcaYrCxZGGOMycqShTHGmKwsWRhjjMnKkoUxxpisLFkYc5CJyL0i8n+HuOw6EfnIgW7HmFyzZGGMMSYrSxbGGGOysmRhRiSv+ecGEXldRLpE5G4RGSsiT4lIh4gsEJHRactfJCJviMguEXlORKakzZsuIku99f4bKBrwXBeKyDJv3RdF5IT9jPkLIrJGRHaIyBMicoRXLiLyQxHZJiJt3mua6s07X0RWerFtFJHr9+sNMyOeJQszkl0KnA0cDXwUeAr4OlCD+218BUBEjgYeAK4DxgDzgD+KSEhEQsAfgN8AVcDvve3irXsScA9wDVAN/AJ4QkTC+xKoiHwI+HfgMqAOeA940Jt9DnCm9zpGAZcDrd68u4FrVLUcmAo8uy/Pa0yKJQszkv1UVbeq6kbgb8DLqvqqqkaAx4Dp3nKXA0+q6tOqGgO+DxQDpwGnAkHgR6oaU9WHgVfSnuMLwC9U9WVVTajqfUDEW29ffAq4R1WXevHNBWaJSAMQA8qBYwBR1VWqutlbLwYcKyIVqrpTVZfu4/MaA1iyMCPb1rTpngyPy7zpI3BH8gCoahLYAIz35m3U3c/I+V7a9ETgq14T1C4R2QVM8NbbFwNj6MTVHsar6rPAfwE/A7aKyJ0iUuEteilwPvCeiDwvIrP28XmNASxZGDMUm3A7fcD1EeB2+BuBzcB4ryzlyLTpDcCtqjoq7Vaiqg8cYAyluGatjQCq+hNVnQEch2uOusErf0VVLwZqcc1lD+3j8xoDWLIwZigeAi4QkQ+LSBD4Kq4p6UXgJSAOfEVEAiLyD8ApaeveBfyziHzA64guFZELRKR8H2P4HfA5EZnm9Xf8P1yz2ToROdnbfhDoAnqBhNen8ikRqfSaz9qBxAG8D2YEs2RhTBaq+hZwJfBTYDuuM/yjqhpV1SjwD8BngZ24/o1H09ZtwvVb/Jc3f4237L7G8AxwM/AIrjYzCbjCm12BS0o7cU1Vrbh+FYBPA+tEpB34Z+91GLPPxC5+ZIwxJhurWRhjjMnKkoUxxpisLFkYY4zJypKFMcaYrCxZGGOMySqQ7wBypaamRhsaGvIdhjHGDCtLlizZrqpjBpYftsmioaGBpqamfIdhjDHDioi8l6ncmqGMMcZkZcligJffaWV5c1u+wzDGmIJiyWKAb/5hBXc8vybfYRhjTEE5bPssMonFYjQ3N9Pb2zvoMt88vQK/T1i1atUhjOzgKioqor6+nmAwmO9QjDGHiRGVLJqbmykvL6ehoYHdzyjdL7itE5/AUWPKMs4vdKpKa2srzc3NNDY25jscY8xhYkQ1Q/X29lJdXT1oogDwCQzncyuKCNXV1XutPRljzL4aUckC2GuiSM1PMoyzBdlfozHG7KsRlyyyyWXNYteuXdx+++37vN7555/Prl27chCRMcYMjSWLAUSEXF3jY7BkkUjs/eJl8+bNY9SoUTmJyRhjhmJEdXAPhQ9I5qhmcdNNN7F27VqmTZtGMBikrKyMuro6li1bxsqVK7nkkkvYsGEDvb29XHvttcyZMwfo/zd6Z2cns2fP5owzzuDFF19k/PjxPP744xQXF+cmYGOM8eQsWYjIBODXwDggCdypqj8Wke/gLjPZ4i36dVWd560zF7gad53gr6jqX7zyGcC9QDEwD7hWD/Dw/5Y/vsHKTe17lEfjSeJJpSTk3+dtHntEBd/+6HGDzv/e977HihUrWLZsGc899xwXXHABK1as6Bu1dM8991BVVUVPTw8nn3wyl156KdXV1bttY/Xq1TzwwAPcddddXHbZZTzyyCNceaVdKdMYk1u5rFnEga+q6lLv4vRLRORpb94PVfX76QuLyLG4awofBxwBLBCRo1U1AdwBzAEW4ZLFecBTuQpcD1EH9ymnnLLb8Naf/OQnPPbYYwBs2LCB1atX75EsGhsbmTZtGgAzZsxg3bp1hyRWY8zIlrNkoaqbcReWR1U7RGQVMH4vq1wMPKiqEeBdEVkDnCIi64AKVX0JQER+DVzCASaLwWoAW9p6aOmIcnx95YFsfkhKS0v7pp977jkWLFjASy+9RElJCWeddVbG4a/hcLhv2u/309PTk/M4jTHmkHRwi0gDMB142Sv6koi8LiL3iMhor2w8sCFttWavbLw3PbA8V7GiaE46ucvLy+no6Mg4r62tjdGjR1NSUsKbb77JokWLDvrzG2PM/sp5shCRMuAR4DpVbcc1KU0CpuFqHj9ILZphdd1LeabnmiMiTSLS1NLSkmmRrHzes+Wik7u6uprTTz+dqVOncsMNN+w277zzziMej3PCCSdw8803c+qppx78AIwxZj/ldDSUiARxieJ+VX0UQFW3ps2/C/iT97AZmJC2ej2wySuvz1C+B1W9E7gTYObMmfu1u0/9oc3VLA7+n9t+97vfZSwPh8M89VTmlrVUv0RNTQ0rVqzoK7/++usPenzGGJNJzmoW4va6dwOrVPW2tPK6tMU+BqT2fk8AV4hIWEQagcnAYq/vo0NETvW2+Rng8dzF7e5zNXzWGGOGo1zWLE4HPg0sF5FlXtnXgU+IyDRcU9I64BoAVX1DRB4CVuJGUn3RGwkF8C/0D519ihyOhPKRXrMwxhgDuR0N9QKZ23Hm7WWdW4FbM5Q3AVMPXnSDS/VZWKowxph+drqPAVJ9FkmrWRhjTB9LFgOk+iwsVxhjTD9LFgP4rGZhjDF7sGQxQC5rFvt7inKAH/3oR3R3dx/kiIwxZmgsWQzgk9yNhrJkYYwZruwU5QOkhm8lc7Dt9FOUn3322dTW1vLQQw8RiUT42Mc+xi233EJXVxeXXXYZzc3NJBIJbr75ZrZu3cqmTZv44Ac/SE1NDQsXLsxBdMYYM7iRmyyeugm2LN+jOKjKUdEE4aAPfPtY8Rp3PMz+3qCz009RPn/+fB5++GEWL16MqnLRRRfx17/+lZaWFo444giefPJJwJ0zqrKykttuu42FCxdSU1OzbzEZY8xBYM1QA/RdvjrH/dvz589n/vz5TJ8+nZNOOok333yT1atXc/zxx7NgwQJuvPFG/va3v1FZmfuz3xpjTDYjt2YxSA0gmVTe2dRGXWURY8qLcvb0qsrcuXO55ppr9pi3ZMkS5s2bx9y5cznnnHP41re+lbM4jDFmKKxmMUAuzzqbforyc889l3vuuYfOzk4ANm7cyLZt29i0aRMlJSVceeWVXH/99SxdunSPdY0x5lAbuTWLQYgIguRk6Gz6Kcpnz57NJz/5SWbNmgVAWVkZv/3tb1mzZg033HADPp+PYDDIHXfcAcCcOXOYPXs2dXV11sFtjDnk5HA9Yd7MmTO1qalpt7JVq1YxZcqUrOuu2NhGdWmIulHFuQov54b6Wo0xJp2ILFHVmQPLrRkqA5/kZuisMcYMV5YsMhAR1C5oYYwxfSxZZCBWszDGmN2MuGQxlD4an8iwvvjRcI7dGFOYRlSyKCoqorW1NevOVBi+pyhXVVpbWykqyt1/RIwxI8+IGjpbX19Pc3MzLS0te12upSOCAD0t4UMT2EFWVFREfX19vsMwxhxGRlSyCAaDNDY2Zl3u1l++TE8swSP/Mu0QRGWMMYVvRDVDDVU44CMST+Q7DGOMKRiWLDIIB31EYjYeyhhjUoaULETkWhGpEOduEVkqIufkOrh8CQf8ROKWLIwxJmWoNYvPq2o7cA4wBvgcMPiFG4Y5a4YyxpjdDTVZpK7ycD7wK1V9La3ssOOShdUsjDEmZajJYomIzMcli7+ISDmH8Z+cw0G/9VkYY0yaoQ6dvRqYBryjqt0iUoVrijospZqhVBWRw7YCZYwxQzbUmsUs4C1V3SUiVwLfBNpyF1Z+hQM+kgpxO5mgMcYAQ08WdwDdInIi8DXgPeDXOYsqz8IBP4D1WxhjjGeoySKu7oRKFwM/VtUfA+V7W0FEJojIQhFZJSJviMi1XnmViDwtIqu9+9Fp68wVkTUi8paInJtWPkNElnvzfiI5bhsKB93bEonZiChjjIGhJ4sOEZkLfBp4UkT8QDDLOnHgq6o6BTgV+KKIHAvcBDyjqpOBZ7zHePOuAI4DzgNu954HXM1mDjDZu503xLj3SzjgJQurWRhjDDD0ZHE5EMH932ILMB74z72toKqbVXWpN90BrPLWuxi4z1vsPuASb/pi4EFVjajqu8Aa4BQRqQMqVPUlr3bz67R1csKaoYwxZndDShZegrgfqBSRC4FeVR1yn4WINADTgZeBsaq62dvuZqDWW2w8sCFttWavbLw3PbA80/PMEZEmEWnKdmbZvemvWVgzlDHGwNBP93EZsBj4R+Ay4GUR+fgQ1y0DHgGu8/4FPuiiGcp0L+V7FqreqaozVXXmmDFjhhJeRv19FlazMMYYGPr/LL4BnKyq2wBEZAywAHh4byuJSBCXKO5X1Ue94q0iUqeqm70mpm1eeTMwIW31emCTV16foTxnrBnKGGN2N9Q+C18qUXhas63rjVi6G1ilqrelzXoCuMqbvgp4PK38ChEJi0gjriN7sddU1SEip3rb/EzaOjlhzVDGGLO7odYs/iwifwEe8B5fDszLss7puNFTy0VkmVf2ddwJCB8SkauB9bimLVT1DRF5CFiJG0n1RVVN7a3/BbgXKAae8m4501ezsGYoY4wBhpgsVPUGEbkUlwAEuFNVH8uyzgsMfrLBDw+yzq3ArRnKm4CpQ4n1YCgK2tBZY4xJN+TLqqrqI7j+h8Nef5+FNUMZYwxkSRYi0kHmkUcCqKpW5CSqPAtbzcIYY3az12Shqns9pcfhqq+D2073YYwxgF2DOyMbOmuMMbuzZJFByKtZ9NpoKGOMASxZZOT3CUG/WAe3McZ4LFkMIhzwWzOUMcZ4LFkMInVpVWOMMfvwP4sRI9IJvW0uWVifhTHGAJYsdqcKt8+C+pmEg1dZM5QxxnisGSqdCDSeCWufodiv1gxljDEeSxYDTf4I9LZxPKutZmGMMR5LFgMd9UEQP6fEl1ifhTHGeCxZDFQ8CiZ8gBnRJmuGMsYYjyWLTCZ/hIbYGkqi2/MdiTHGFARLFpm872wATuxtynMgxhhTGCxZZDLueNoC1ZzY+wrxhPVbGGOMJYtMROg48mz+TpewcMkb+Y7GGGPyzpLFIOpmX09I4vQ8d1u+QzHGmLyzZDEI/5jJvFN3Aed0/ZEVb72V73CMMSavLFnsRd1F3yJAgu1//o98h2KMMXllyWIvyuqOZnnNbGbteIJNTU/kOxxjjMkbSxZZTLjs//OujKf2T58l+er9+Q7HGGPywpJFFjVj61l7wUMsShyD7/H/Az+eBo9/CbasyHdoxhhzyFiyGILzZx7NbyZ9n+8mPsvmcCO68nG464Pw0s8gaf/DMMYc/ixZDIGI8G//cBIvVl/KrHX/xGWh22muPg3+8nW480xY9HPoynJqkJa3oXXtoQnYGGMOMlHVfMeQEzNnztSmpoN7uo5kUnly+WZ++uxq3t7aweWBv/Hlsmep730bfAF3mpCJp8H6l2DTq3DMBTDrS/DaA/C3H7iLK838PJw1F0qr3UZ3vgfP/wfsWg+9u6DhTDjzeiipOqixG2PMUIjIElWduUd5rpKFiNwDXAhsU9WpXtl3gC8ALd5iX1fVed68ucDVQAL4iqr+xSufAdwLFAPzgGt1CEHnIlmkqCpvbGrn4SXN/GbRe5xcsoVbJr7O5C3z8HVthVFHwtipsPppSMbcSidcAeEyaPoVBIth2qegZjIsuAVQt3ywCN79K4TL4cyvwSlfgEA4J6+hYKjC2meh8e/BbxduNCbf8pEszgQ6gV8PSBadqvr9AcseCzwAnAIcASwAjlbVhIgsBq4FFuGSxU9U9alsz5/LZJFuxcY2vvbw66zc3I5fkpxWG2fU2Ik01JQypbiNadufoLTxA1SceCEiAtvehBd+CCsecYnkyNPgYz+H0RPdBre+AU9/C9YsgFET4YzroPY4qKyHUAmIH3a+C9tXQ/X7oO5Ed4W/4US1P+bXfw+P/hN86GZXozLG5NUhTxbekzYAfxpCspgLoKr/7j3+C/AdYB2wUFWP8co/AZylqtdke+5DlSwAEkll2YadvLC6lSXrd/JeaxfNO3tIJPvf29KQn3GVRVSXhqkqDTEx3M5k1tNRdzpV5cWUhAIUB/0cPa6M2vIiWPMMzL8ZtmU5N9XY4+Gov3c1kLKxcNzHoKw2x694P6nCvOth41K46o8QKILbPwCtayBUBl95tXBjN9mpQrTT1YzNsDVYsshHvf9LIvIZoAn4qqruBMbjag4pzV5ZzJseWF5Q/D5hxsQqZkzs72eIJZJs3NnDutYu3mvt5t3tXWxt76W1K8ralk5e6Yqzs7uG5NI9TyVy3BEVnFBfy5hJd3PU5C0coduoSbZQEYhT6k8SqmnAP2YybHgZXr0fmu6BRBSScfjzXGj8O/AFId4Lkz4I0z8DxaNhxzuQiEDVURAqPZRvkfPyL+CVX7rpeTe4JNe6Bj78bVh4Kzz3PbjQzsU1LCWT8MjnYfUCuHo+jD023xGZg+xQJ4s7gH8D1Lv/AfB5IFM7iu6lPCMRmQPMATjyyCMPNNYDEvT7aKgppaFm8J1yIqm09cTY0RWlJ5qgMxLn1Q07ef6tFua/sYUd3VFcxa/Mu/UL+ZspDh1JSeiblBT5KQkFmF68jc+XvsjEXS8i/hBoEp75Liz8d/D5XfJIKRsLpWMgXAE9O6B7B1TUwZgpUFQB8Yj3QkpcEtqxFjq2wsRZMPlc6N4OGxa7pNN4JtROAfG59dqaoasFKsa7xJSIQHOTGz32/gvcjuSv/wlvPulqRqdfBx2b4ZW7YfI5LokEi4f2RieTbmBAdysgUNXoXutQdO9wzXnjprrX0b4Zlj/ktlN3Aow5BkprXZNZ+ybo2ALjjodAKPP2VGHzMtfEuHEpTP80nHA5+Apg0GH7Jnjiy5CIwd/fCA2n7992EjHXVFo7Zff+tGe/C2885r4vD34S5ix0Byjp1j7rvo/xiGt2fP/s4deEmqIK6xdB5XjXRzkCHNJmqMHmDfdmqFxJJJUdXVFauyJs74iyvTPC9s4I3dEE3dEEPdF433R3NM7Kze1sbY9QV1nErEnVTD9yNJNoZuL6PxD2g9YeS7i4lOKOdwm0rUe6WyHS7n7UxaOhfaPrU4n3gD8MKMS8BFPV6EZorV8EsW5XFq50yyaiQ3tBNUfDPz3jdij3fRTWvwiX3w9TLnRDj28/1SUZf8j11xRVuD6anp0Q7XJ9NsFiSCbcDqe3zSU6TfuvS7DU7cjKx7lL5LZtdIkuUOT6fcIVLt62Dd4fK9U9X+0U91gHXErXF3Q7xWine1w0Ct5/vtt2tNNtr+ool+xWPOJqb76g24nsXAfjZ8L7z4OycS7BblnuYh93gtvJtG+Ezm0ucVfUQW+7S7YoFFe5Jp1Ux3+kI+3WCeVjXX9WrNuNvou0u2Q2diqUVLvYetugZRXM/ybEo26QRedWmPABmPJRd9+xxcXas9Nto3wc1E2H0Q1u0EW0Cza/Bu/9D6x8wr3noxvh3FuhvA5WPg7/8yOY8Tk48RNw7wXuoGLG59xnvXmZa1ZtXuxec6AItr/t+uuOucAdbFRPcgm7cxtsWube87FT3We+4133eXW3ulgmnOLev4GJRhW2rYJtK93BUOV49z3y+d289o3uu1Ix3h3YpA4Aiirc+9y13T0OlbjXVV7nXv9A21e75tR3nnO/k1lfhDP+1W3nQKi670akw31OQz1gOsgKpc+iTlU3e9P/CnxAVa8QkeOA39Hfwf0MMNnr4H4F+DLwMq6D+6epEVR7czgki30VSyRZsHIrj726kaXrd7K9c/CdeNAvlIUDlIYDhAI+Qn5f331xyE95UYCioB+/CH6f4PMJAZ9Q5o8xqWcF8dKx9Fa+jyKJM7btdcp6N+IXQf1BoiXjiBdVU9q7hdLuDRAoIlFSS/e4k6GokoBPCEV2ULH5f+iefDF+v4+Az0cg3kHx5pcJb1xEoL0ZX7QD0QSUjEZCpUi8F4l2ux9/IAxFlW6nmLolorD5dbez6NrudmoVR7iBAPGI2+FEu13NoLgKGs5wtYfmxa7mM+EUOOkqlxC2vOb+F9PWDLEeN3KtpAreng9v/9klrFCpq9XEe93Op/FMmHopHHOh28brD7paXdv6/je+8kj3/K1r6askhyvcTrrvw/EGMkQ79vzgxO92bKFSt9NPxr1tVLqdVduGzB/4uBPg479yO9CmX8Grv92zPyy17d5dg3xpSl3im3i6a1LcntaEOuWjbvv+ICy5D/50XVoSF5fETvwEnHy1e69euRteucs1Q6aEyjO/5kxKa6G0xn0O/qB7z9o3uls6f9gl884tLhmCOzjwBSHWlf15ike774qIq1V1t3oHCZVw1o3u+/b6g26bE09zB0Tb33YHKSXVLqGHK9zn1bMTdm1wyT1Y7N6HSIf77CMd7kAhNXoS+hN+IuZq56mDstGN7vuIuOSpCbetWLerHXdtgxvWDr2GPUA+RkM9AJwF1ABbgW97j6fhfiXrgGvSksc3cE1SceC61IgnEZlJ/9DZp4Av53vo7HCgqmzc1cPOrhgdkRidvXE6I+7W4U13ReJ09saJJJLE4kmiiSTReJLuaIKO3hi9sSRJVRJJJalKPKn0xhL0xvL3r/WAzyWvvnu/r++xT4SAP32+D5+437kg+AQQdy+AT1wSLAn5KQ0FQNz7VhTwU1YUIOj3EU8kiSWVeCJJPKHEkkoyqVQUB6gpCxP2Q3FvC1EJ0R2o9N4rUJSyUICyogDR3m4S7VtIhCooG1VDaSiAxDoJd2+lI1RLzF9MqT/O6EQr0WA53b4K/H6hRBKEtZtkIk4imSTiKyUqYeLq/vMT9sWpjTYTDhfjqzkKxceu7ZuR1tVUSReV/gjJcAXRcBWdo48lTmC3zzPU2UzJjlV0F42ju7SeytE1jK0oprtjBx3vLiHQtZVyf4xQURGRmqnEqybj8wfd+5eMU7H2ceIqtNScipaNpaokREnYTzyh0L2d4ugOipOdJKuPJhEeRTyhxJNJFAj6fPj9QqBjI8HmRfjam5HOLWjlBBJ100nGIiS3LCcZ7SVWOZFExZEUjx5HUTiErHsB3nne7bR9fjQRcwcBRZUkJ32I5BEnoV2tyK71SOvbyPbVaFktidrjCYbC+Ha+63a81ZNcLSPSCZE2KKlxtapol6thdGxyO99U8hS/S1Dl49xQ+PKxrnzTq7D8YTeCcdcGGHO0q0F173AJPdLhtllUCZUTXK0h1uOSabjcJYRweX8NJ1UjbNvgYguEXTLyh1xiaF3rbiJueZ/fbStQ1F8j+sh3XA1pP+SlZpFPIz1Z5FIyqUTiSbqjcWIJJaFKInWfTJJIQjyZJOndJ5Iu0fTfux1vIplaR/se983fbfkM8xIZtuntyFPlsYQC3s5bUztxN63eDj2eUHpiCboi7ghdROiNuf6jWDxJwO8j6BdX8/ELQb9LQG09cXZ0RUgb8IZP+hMQCtG0S/KG/D73nhyeP7eDIuATEt5nMxiX/MV9hrDXZQdbv6o0TEVxwH1W3meGt62kqndLm066aYCioJ9wwEdR0E9x0E9R0NXEAaLxJLGEupq4t91Urdwngl/om07Nl7Rpn+Bq9wEf4YCfUMBHUtUd0MWTBP0+Qt53MOD3objve8KLrzeeoKPXHQz++PJp7nu4HwppNJQZ5nw+oTjk7/uRjFSpI/TUD10GtKFH40k6I3FKQn6Kgn6SSWVXT4zuaJyAz4fPB35vh9Ebc/1OIkLQ53YSPbEE8YT21ZbSa1Q+EaLxJF3ROF2RBF1Rl+xqy8OUh4Ps7I6yo9s1W7gdlfQ9X9/Oyyfe87t4W7uibG3vpTwcoH50CcGAsLMrRmckTlK1L+GmaieqbudWHHKvbWe3e22p+Hqiaa/J72p6Qb97slQtI+4dKMS92lvAqy0GvSTtdoyCKq5m3BtH0d1qit4dgnst6Z9FerkAXZE4LZ0R2nvcdvoTBH3vkduBezv3tB06QG886dWu3a2lM0ZPNIGIEA74CPiEpPZ/N1L36WXJpEt0qXJNmx9LJInEk7sNuw/43LZj3vzBEmTI76OiOEB5UZDeeIKS0MHdvVuyMGY/+X2CP+OAPScU8FGVNnLK5xOqSkNUlQ4ymuogOrJ6/5ogTGGIJ1yzsM9LQqnkp9pfa5b02kuGg5WDzZKFMcYUmIDX1DSQeP1ygTxU6gtgALgxxphCZ8nCGGNMVoftaCgRaQHe28/Va4AsF6goKBZvblm8uTXc4oXhF/O+xDtRVccMLDxsk8WBEJGmTEPHCpXFm1sWb24Nt3hh+MV8MOK1ZihjjDFZWbIwxhiTlSWLzO7MdwD7yOLNLYs3t4ZbvDD8Yj7geK3PwhhjTFZWszDGGJOVJYs0InKeiLwlImtE5KZ8xzOQiEwQkYUiskpE3hCRa73yKhF5WkRWe/ejs23rUBIRv4i8KiJ/8h4XeryjRORhEXnTe69nFXLMIvKv3vdhhYg8ICJFhRSviNwjIttEZEVa2aDxichc7zf4loicWyDx/qf3fXhdRB4TkVGFHG/avOtFREWkJq1sv+K1ZOERET/wM2A2cCzwCREptGtDxnGXop0CnAp80YvxJuAZVZ2MuxZIoSW6a4FVaY8LPd4fA3/2Lrp1Ii72goxZRMYDXwFmeteN8QNXUFjx3gucN6AsY3ze9/kK4Dhvndu93+ahdC97xvs0MFVVTwDeBuZCQceLiEwAzgbWp5Xtd7yWLPqdAqxR1XdUNQo8CFyc55h2o2/A6IgAAASXSURBVKqbVXWpN92B24mNx8V5n7fYfcAl+YlwTyJSD1wA/DKtuJDjrQDOBO4GUNWoqu6igGPGneOtWEQCQAmwiQKKV1X/CuwYUDxYfBcDD6pqRFXfBdbgfpuHTKZ4VXW+qnpXmmIRUO9NF2S8nh8CX2P3S1Hvd7yWLPqNB9IvM9bslRUk7yqE03FXEBybuoiUd1+bv8j28CPcFzb9ikmFHO9RQAvwK6/p7JciUkqBxqyqG4Hv444eNwNtqjqfAo03zWDxDYff4edxF2KDAo1XRC4CNqrqawNm7Xe8liz6ZTq/b0EOFRORMuAR3BUF27Mtny8iciGwTVWX5DuWfRAATgLuUNXpQBcF0uSUidfWfzHQiLskcamIXJnfqA5IQf8OvSt6xoH7U0UZFstrvCJSAnwD+Fam2RnKhhSvJYt+zcCEtMf1uOp8QRGRIC5R3K+qj3rFW0WkzptfB2zLV3wDnA5cJCLrcM16HxKR31K48YL7HjSr6sve44dxyaNQY/4I8K6qtqhqDHgUOI3CjTdlsPgK9ncoIlcBFwKfSru0cyHGOwl38PCa99urB5aKyDgOIF5LFv1eASaLSKOIhHCdQE/kOabdiIjg2tJXqeptabOeAK7ypq8CHj/UsWWiqnNVtV5VG3Dv57OqeiUFGi+Aqm4BNojI+72iDwMrKdyY1wOnikiJ9/34MK4vq1DjTRksvieAK0QkLCKNwGRgcR7i242InAfcCFykqt1pswouXlVdrqq1qtrg/faagZO87/b+x6ve5RLtpgDn40Y6rAW+ke94MsR3Bq7K+DqwzLudD1TjRpSs9u6r8h1rhtjPAv7kTRd0vMA0oMl7n/8AjC7kmIFbgDeBFcBvgHAhxQs8gOtPiXk7rqv3Fh+uCWUt8BYwu0DiXYNr60/97n5eyPEOmL8OqDnQeO0f3MYYY7KyZihjjDFZWbIwxhiTlSULY4wxWVmyMMYYk5UlC2OMMVlZsjCmwIjIWakz9BpTKCxZGGOMycqShTH7SUSuFJHFIrJMRH7hXbejU0R+ICJLReQZERnjLTtNRBalXQ9htFf+PhFZICKveetM8jZfJv3X1Ljf+3e2MXljycKY/SAiU4DLgdNVdRqQAD4FlAJLVfUk4Hng294qvwZuVHc9hOVp5fcDP1PVE3HndNrslU8HrsNdW+Uo3Hm2jMmbQL4DMGaY+jAwA3jFO+gvxp0MLwn8t7fMb4FHRaQSGKWqz3vl9wG/F5FyYLyqPgagqr0A3vYWq2qz93gZ0AC8kPuXZUxmliyM2T8C3Keqc3crFLl5wHJ7O5/O3pqWImnTCey3avLMmqGM2T/PAB8XkVrou6b0RNxv6uPeMp8EXlDVNmCniPydV/5p4Hl11yJpFpFLvG2EvWsRGFNw7GjFmP2gqitF5JvAfBHx4c74+UXcxZKOE5ElQBuuXwPcabh/7iWDd4DPeeWfBn4hIt/1tvGPh/BlGDNkdtZZYw4iEelU1bJ8x2HMwWbNUMYYY7KymoUxxpisrGZhjDEmK0sWxhhjsrJkYYwxJitLFsYYY7KyZGGMMSYrSxbGGGOy+l899IyHQyLuDQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 2795.2541 - val_loss: 1861.3834\n",
      "Epoch 2/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1713.3753 - val_loss: 1693.3213\n",
      "Epoch 3/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1632.5563 - val_loss: 1665.8006\n",
      "Epoch 4/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1610.2520 - val_loss: 1625.4861\n",
      "Epoch 5/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1595.7219 - val_loss: 1614.3661\n",
      "Epoch 6/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1566.5795 - val_loss: 1588.5247\n",
      "Epoch 7/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1553.2104 - val_loss: 1586.2087\n",
      "Epoch 8/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1545.6888 - val_loss: 1568.7650\n",
      "Epoch 9/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1540.3158 - val_loss: 1565.7728\n",
      "Epoch 10/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1535.5902 - val_loss: 1562.1563\n",
      "Epoch 11/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1532.4260 - val_loss: 1560.4161\n",
      "Epoch 12/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1529.5421 - val_loss: 1549.3037\n",
      "Epoch 13/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1527.0356 - val_loss: 1552.8836\n",
      "Epoch 14/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1524.8803 - val_loss: 1555.8974\n",
      "Epoch 15/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1520.6895 - val_loss: 1555.9122\n",
      "Epoch 16/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1506.7884 - val_loss: 1540.1236\n",
      "Epoch 17/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1502.4196 - val_loss: 1535.3086\n",
      "Epoch 18/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1500.3704 - val_loss: 1528.4228\n",
      "Epoch 19/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1497.9537 - val_loss: 1522.8110\n",
      "Epoch 20/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1497.0188 - val_loss: 1525.8888\n",
      "Epoch 21/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1495.1039 - val_loss: 1531.2872\n",
      "Epoch 22/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1494.3571 - val_loss: 1532.9008\n",
      "Epoch 23/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1492.8409 - val_loss: 1519.6261\n",
      "Epoch 24/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1492.0440 - val_loss: 1518.0831\n",
      "Epoch 25/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1490.1571 - val_loss: 1520.1300\n",
      "Epoch 26/500\n",
      "  224/50000 [..............................] - ETA: 2:43 - loss: 1506.5798"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PazderaAaron\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.232469). Check your callbacks.\n",
      "  % (hook_name, delta_t_median), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1490.2286 - val_loss: 1515.6649\n",
      "Epoch 27/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1489.1733 - val_loss: 1518.1329\n",
      "Epoch 28/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1488.4406 - val_loss: 1514.0714\n",
      "Epoch 29/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1487.0350 - val_loss: 1519.5809\n",
      "Epoch 30/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1486.6785 - val_loss: 1511.4028\n",
      "Epoch 31/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1486.0084 - val_loss: 1515.3053\n",
      "Epoch 32/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1484.5034 - val_loss: 1518.5564\n",
      "Epoch 33/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1484.8410 - val_loss: 1514.2857\n",
      "Epoch 34/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1484.1485 - val_loss: 1513.8092\n",
      "Epoch 35/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1483.7986 - val_loss: 1513.1727\n",
      "Epoch 36/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1482.8910 - val_loss: 1517.6176\n",
      "Epoch 37/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1482.5765 - val_loss: 1507.5198\n",
      "Epoch 38/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1482.7581 - val_loss: 1507.5043\n",
      "Epoch 39/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1481.3901 - val_loss: 1512.6481\n",
      "Epoch 40/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1481.3949 - val_loss: 1515.2519\n",
      "Epoch 41/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1480.9314 - val_loss: 1516.5842\n",
      "Epoch 42/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1480.4561 - val_loss: 1513.4452\n",
      "Epoch 43/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1480.2037 - val_loss: 1526.1043\n",
      "Epoch 44/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1479.5432 - val_loss: 1506.6153\n",
      "Epoch 45/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1478.7569 - val_loss: 1509.9737\n",
      "Epoch 46/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1478.6573 - val_loss: 1506.4772\n",
      "Epoch 47/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1478.4992 - val_loss: 1505.9658\n",
      "Epoch 48/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1478.6244 - val_loss: 1505.1575\n",
      "Epoch 49/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1477.9343 - val_loss: 1504.9314\n",
      "Epoch 50/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1477.9640 - val_loss: 1505.5145\n",
      "Epoch 51/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1476.9778 - val_loss: 1508.0861\n",
      "Epoch 52/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1477.0012 - val_loss: 1507.7993\n",
      "Epoch 53/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1476.4643 - val_loss: 1527.4575\n",
      "Epoch 54/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1476.2519 - val_loss: 1504.7705\n",
      "Epoch 55/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1475.9895 - val_loss: 1504.0622\n",
      "Epoch 56/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1475.8374 - val_loss: 1505.6891\n",
      "Epoch 57/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1475.0817 - val_loss: 1516.3787\n",
      "Epoch 58/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1475.7492 - val_loss: 1506.0943\n",
      "Epoch 59/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1474.6888 - val_loss: 1509.6265\n",
      "Epoch 60/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1474.5008 - val_loss: 1501.2878\n",
      "Epoch 61/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1475.7905 - val_loss: 1500.0476\n",
      "Epoch 62/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1474.1112 - val_loss: 1503.2498\n",
      "Epoch 63/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1473.2714 - val_loss: 1503.0592\n",
      "Epoch 64/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1473.1840 - val_loss: 1500.7996\n",
      "Epoch 65/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1473.4166 - val_loss: 1501.9351\n",
      "Epoch 66/500\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1472.6219 - val_loss: 1510.2432\n",
      "Epoch 67/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1472.6770 - val_loss: 1506.6995\n",
      "Epoch 68/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1472.1073 - val_loss: 1514.0429\n",
      "Epoch 69/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1471.7653 - val_loss: 1507.7108\n",
      "Epoch 70/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1471.9240 - val_loss: 1504.0334\n",
      "Epoch 71/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1471.1487 - val_loss: 1500.0882\n",
      "Epoch 72/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1471.1722 - val_loss: 1500.0453\n",
      "Epoch 73/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1472.0580 - val_loss: 1503.0559\n",
      "Epoch 74/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1470.4282 - val_loss: 1501.6047\n",
      "Epoch 75/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1470.5083 - val_loss: 1497.9009\n",
      "Epoch 76/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1470.4449 - val_loss: 1498.8161\n",
      "Epoch 77/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1470.2977 - val_loss: 1497.3405\n",
      "Epoch 78/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1470.1493 - val_loss: 1500.4712\n",
      "Epoch 79/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1469.7187 - val_loss: 1508.2046\n",
      "Epoch 80/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1469.4640 - val_loss: 1496.4128\n",
      "Epoch 81/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1468.9329 - val_loss: 1497.7708\n",
      "Epoch 82/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1469.3508 - val_loss: 1499.0105\n",
      "Epoch 83/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1468.6177 - val_loss: 1496.9765\n",
      "Epoch 84/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1467.9274 - val_loss: 1497.5571\n",
      "Epoch 85/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1469.3469 - val_loss: 1498.4012\n",
      "Epoch 86/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1469.1010 - val_loss: 1496.0196\n",
      "Epoch 87/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1467.8642 - val_loss: 1496.5472\n",
      "Epoch 88/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1467.8133 - val_loss: 1508.3821\n",
      "Epoch 89/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1468.0515 - val_loss: 1495.8420\n",
      "Epoch 90/500\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1468.0500 - val_loss: 1502.9221\n",
      "Epoch 91/500\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1467.2356 - val_loss: 1496.6658\n",
      "Epoch 92/500\n",
      "50000/50000 [==============================] - 67s 1ms/step - loss: 1467.1507 - val_loss: 1502.4366\n",
      "Epoch 93/500\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1468.0892 - val_loss: 1494.4775\n",
      "Epoch 94/500\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1466.8127 - val_loss: 1501.6645\n",
      "Epoch 95/500\n",
      "50000/50000 [==============================] - 66s 1ms/step - loss: 1466.6524 - val_loss: 1497.2085\n",
      "Epoch 96/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1466.6273 - val_loss: 1497.9200\n",
      "Epoch 97/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1467.7292 - val_loss: 1512.8623\n",
      "Epoch 98/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1466.9939 - val_loss: 1498.7957\n",
      "Epoch 99/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1466.3737 - val_loss: 1504.0868\n",
      "Epoch 100/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1466.9110 - val_loss: 1495.3136\n",
      "Epoch 101/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1465.5958 - val_loss: 1495.9878\n",
      "Epoch 102/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1465.5199 - val_loss: 1501.2630\n",
      "Epoch 103/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1466.0977 - val_loss: 1494.8592\n",
      "Epoch 104/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1465.9022 - val_loss: 1510.0243\n",
      "Epoch 105/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1465.2759 - val_loss: 1500.8817\n",
      "Epoch 106/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1465.3854 - val_loss: 1498.5879\n",
      "Epoch 107/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1464.7494 - val_loss: 1497.0704\n",
      "Epoch 108/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1465.1469 - val_loss: 1494.3713\n",
      "Epoch 109/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1465.0759 - val_loss: 1493.3901\n",
      "Epoch 110/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1464.5488 - val_loss: 1501.3266\n",
      "Epoch 111/500\n",
      "50000/50000 [==============================] - 63s 1ms/step - loss: 1464.0547 - val_loss: 1496.7486\n",
      "Epoch 112/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1464.3382 - val_loss: 1496.9626\n",
      "Epoch 113/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1465.0668 - val_loss: 1505.3325\n",
      "Epoch 114/500\n",
      "50000/50000 [==============================] - 64s 1ms/step - loss: 1463.9895 - val_loss: 1494.4622\n",
      "Epoch 115/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1464.1227 - val_loss: 1504.0989\n",
      "Epoch 116/500\n",
      "50000/50000 [==============================] - 62s 1ms/step - loss: 1463.7162 - val_loss: 1505.1792\n",
      "Epoch 117/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1463.8602 - val_loss: 1489.4960\n",
      "Epoch 118/500\n",
      "50000/50000 [==============================] - 65s 1ms/step - loss: 1463.6032 - val_loss: 1496.4691\n",
      "Epoch 119/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1463.8027 - val_loss: 1500.2467\n",
      "Epoch 120/500\n",
      "50000/50000 [==============================] - 58s 1ms/step - loss: 1463.8675 - val_loss: 1490.8555\n",
      "Epoch 121/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1462.8163 - val_loss: 1497.7531\n",
      "Epoch 122/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1462.8693 - val_loss: 1493.1618\n",
      "Epoch 123/500\n",
      "50000/50000 [==============================] - 61s 1ms/step - loss: 1462.4299 - val_loss: 1490.0895\n",
      "Epoch 124/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1465.6923 - val_loss: 1509.7679\n",
      "Epoch 125/500\n",
      "50000/50000 [==============================] - 73s 1ms/step - loss: 1462.8769 - val_loss: 1488.9492\n",
      "Epoch 126/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1462.1854 - val_loss: 1496.5669\n",
      "Epoch 127/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1462.5265 - val_loss: 1498.5858\n",
      "Epoch 128/500\n",
      "50000/50000 [==============================] - 60s 1ms/step - loss: 1462.0887 - val_loss: 1490.5407\n",
      "Epoch 129/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1462.8626 - val_loss: 1489.8412\n",
      "Epoch 130/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1463.8771 - val_loss: 1491.6011\n",
      "Epoch 131/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1462.5865 - val_loss: 1490.0543\n",
      "Epoch 132/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1460.6413 - val_loss: 1491.0318\n",
      "Epoch 133/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1461.1869 - val_loss: 1492.5454\n",
      "Epoch 134/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1461.5463 - val_loss: 1488.8085\n",
      "Epoch 135/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.8500 - val_loss: 1489.0386\n",
      "Epoch 136/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1459.7620 - val_loss: 1490.5501\n",
      "Epoch 137/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1459.5438 - val_loss: 1508.1589\n",
      "Epoch 138/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1459.0489 - val_loss: 1503.5989\n",
      "Epoch 139/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1459.1786 - val_loss: 1490.1781\n",
      "Epoch 140/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1459.7729 - val_loss: 1489.5193\n",
      "Epoch 141/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1460.4896 - val_loss: 1492.0082\n",
      "Epoch 142/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.3021 - val_loss: 1488.7319\n",
      "Epoch 143/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1458.7610 - val_loss: 1485.8287\n",
      "Epoch 144/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.1916 - val_loss: 1486.7789\n",
      "Epoch 145/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1458.9062 - val_loss: 1490.3166\n",
      "Epoch 146/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.3829 - val_loss: 1492.1309\n",
      "Epoch 147/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1460.1917 - val_loss: 1492.1884\n",
      "Epoch 148/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1457.7976 - val_loss: 1497.2229\n",
      "Epoch 149/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1458.3488 - val_loss: 1491.1654\n",
      "Epoch 150/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.2788 - val_loss: 1485.6134\n",
      "Epoch 151/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1456.8952 - val_loss: 1499.1861\n",
      "Epoch 152/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1459.5754 - val_loss: 1495.5282\n",
      "Epoch 153/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1457.6933 - val_loss: 1483.8674\n",
      "Epoch 154/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1459.1156 - val_loss: 1495.7587\n",
      "Epoch 155/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.9517 - val_loss: 1496.0526\n",
      "Epoch 156/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1460.0618 - val_loss: 1486.5965\n",
      "Epoch 157/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1457.7938 - val_loss: 1489.0041\n",
      "Epoch 158/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1457.8507 - val_loss: 1484.8258\n",
      "Epoch 159/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1456.3800 - val_loss: 1495.1110\n",
      "Epoch 160/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1460.7967 - val_loss: 1491.7704\n",
      "Epoch 161/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.0102 - val_loss: 1493.1192\n",
      "Epoch 162/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1457.5205 - val_loss: 1485.9492\n",
      "Epoch 163/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.7161 - val_loss: 1531.2467\n",
      "Epoch 164/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.5076 - val_loss: 1485.6489\n",
      "Epoch 165/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1458.8176 - val_loss: 1488.0319\n",
      "Epoch 166/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.6358 - val_loss: 1490.3449\n",
      "Epoch 167/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1458.0715 - val_loss: 1493.5579\n",
      "Epoch 168/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1457.9736 - val_loss: 1487.6287\n",
      "Best Epoch: 152\n",
      "Current Loss: 1483.867426953125\n",
      "Best Loss: 1458.02185390625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 2485.9546 - val_loss: 1805.2025\n",
      "Epoch 2/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1725.2993 - val_loss: 1720.5167\n",
      "Epoch 3/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1678.9259 - val_loss: 1691.6566\n",
      "Epoch 4/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1660.4014 - val_loss: 1702.2592\n",
      "Epoch 5/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1631.5026 - val_loss: 1637.6178\n",
      "Epoch 6/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1602.3020 - val_loss: 1616.5614\n",
      "Epoch 7/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1573.8206 - val_loss: 1603.1049\n",
      "Epoch 8/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1567.5642 - val_loss: 1595.6581\n",
      "Epoch 9/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1563.4367 - val_loss: 1586.6797\n",
      "Epoch 10/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1560.1485 - val_loss: 1587.4521\n",
      "Epoch 11/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1556.9297 - val_loss: 1584.2246\n",
      "Epoch 12/500\n",
      "50000/50000 [==============================] - 56s 1ms/step - loss: 1554.5499 - val_loss: 1580.6822\n",
      "Epoch 13/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1551.8035 - val_loss: 1583.9944\n",
      "Epoch 14/500\n",
      "50000/50000 [==============================] - 59s 1ms/step - loss: 1549.7646 - val_loss: 1588.9852\n",
      "Epoch 15/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1549.4273 - val_loss: 1577.5831\n",
      "Epoch 16/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1546.0880 - val_loss: 1570.8776\n",
      "Epoch 17/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1544.9831 - val_loss: 1575.3351\n",
      "Epoch 18/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1542.9254 - val_loss: 1571.6072\n",
      "Epoch 19/500\n",
      "50000/50000 [==============================] - 55s 1ms/step - loss: 1541.4148 - val_loss: 1569.6594\n",
      "Epoch 20/500\n",
      "50000/50000 [==============================] - 57s 1ms/step - loss: 1540.6327 - val_loss: 1568.4492\n",
      "Epoch 21/500\n",
      "46304/50000 [==========================>...] - ETA: 4s - loss: 1539.3111"
     ]
    }
   ],
   "source": [
    "if train_ed:\n",
    "    # Train some models, save the good ones.\n",
    "    # The feilds in the file name are as follows:\n",
    "    best_loss = np.Infinity\n",
    "    for i in range(num_models):\n",
    "        # Because weights cannot be reset, the model must be rebuilt every time we want to \n",
    "        # train a new one. But, luckily, it's not that bad.\n",
    "        encode_decode = build_encode_decode(i_shape, latent_shape, convlayers, denselayers)\n",
    "        encode_decode.compile(optimizer='adam', loss=mean_squared_error)\n",
    "        \n",
    "        # Train the model\n",
    "        history = encode_decode.fit(x_train, x_train, \\\n",
    "                        validation_data=(x_test, x_test), \\\n",
    "                        validation_split=validation_ratio, \\\n",
    "                        epochs=epoch_num, batch_size=batch_size, shuffle=True, \\\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "        # Evaluate how well we've done\n",
    "        loss_score = min(history.history['val_loss'])\n",
    "        best_epoch = history.history['val_loss'].index(loss_score)\n",
    "    \n",
    "        # Require the model to train for more than 10 epochs, and save it if it's good.\n",
    "        if 10 < best_epoch and best_loss >= loss_score:\n",
    "            best_loss = loss_score\n",
    "            os.makedirs('models/encode-decode', exist_ok=True)\n",
    "            rand = ''.join(random.choices(\"0123456789\", k=10))\n",
    "            fname = f'models/encode-decode/{int(loss_score)}_{side_length}_{latent_shape[1]}_{convlayers}_{denselayers}_{distance_weight}_{mse_weight}_{rand}.h5' \n",
    "            encode_decode.save(fname)\n",
    "            print(f'Saving model: {fname}')\n",
    "        \n",
    "        # Print \n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "        print(f'Current Loss: {loss_score}')\n",
    "        print(f'Best Loss: {best_loss}')\n",
    "        plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Encode-Decode Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ed_model():\n",
    "    # Find the best one\n",
    "    best_split = ''\n",
    "    for root, dirs, files in os.walk(\"models/encode-decode\", topdown=False):\n",
    "        for name in files:\n",
    "            spt = name.split('_')\n",
    "            spt[7] = spt[7][:-3]\n",
    "            nloss = spt[0]\n",
    "            nsidelength = spt[1]\n",
    "            nlatent = spt[2]\n",
    "            nconv = spt[3]\n",
    "            ndense = spt[4]\n",
    "            ndistw = spt[5]\n",
    "            nmsew = spt[6]\n",
    "            nrand = spt[7]\n",
    "            if (int(nsidelength) == side_length) and (int(nlatent) == latent_shape[1]) and (int(nconv) == convlayers) and \\\n",
    "        (int(ndense) == denselayers) and (int(ndistw) == distance_weight) and (int(nmsew) == mse_weight):\n",
    "                best_split = spt\n",
    "    # Load the best one\n",
    "    best = 'models/encode-decode/' + '_'.join(best_split) + '.h5'\n",
    "    print(f'loaded model: {best}')\n",
    "    return load_model(best) if best_split != '' else None\n",
    "\n",
    "def get_ed_model():\n",
    "    try:\n",
    "        global encode_decode\n",
    "        encode_decode\n",
    "    except NameError:\n",
    "        encode_decode = load_ed_model()\n",
    "    return encode_decode\n",
    "\n",
    "encode_decode = load_ed_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now let's define some functions, then try the model out.\n",
    "def download_img(img_url):\n",
    "    return Image.open(urlopen(img_url))\n",
    "\n",
    "def resize_img(img):\n",
    "    convd_img = img.resize((side_length, side_length))\n",
    "    convd_img = convd_img.convert('RGB')\n",
    "    return convd_img\n",
    "\n",
    "def run_ed_model(img):\n",
    "    img_array = np.asarray(img)\n",
    "    img_array = img_array.reshape((1, side_length, side_length, 3))\n",
    "    decoded_tensor = get_ed_model().predict(img_array).reshape((side_length, side_length, 3))\n",
    "    nd = tf.make_ndarray(tf.make_tensor_proto(decoded_tensor))\n",
    "    return nd.astype(int)\n",
    "\n",
    "def get_encoder():\n",
    "    return get_ed_model().get_layer('Encode')\n",
    "\n",
    "def get_decoder():\n",
    "    return get_ed_model().get_layer('Encode')\n",
    "\n",
    "def run_encoder(img):\n",
    "    img_array = np.asarray(img)\n",
    "    img_array = img_array.reshape((1, side_length, side_length, 3))\n",
    "    encoder = get_encoder()\n",
    "    newshape = list(latent_shape)\n",
    "    newshape[0] = 1\n",
    "    encoded_tensor = encoder.predict(img_array).reshape(newshape)\n",
    "    return tuple(encoded_tensor[0])\n",
    "\n",
    "def show_images(img_url, show=False):\n",
    "    # Download image and convert.\n",
    "    original_img = download_img(img_url)\n",
    "    convd_img = resize_img(original_img)\n",
    "    \n",
    "    # Show original image and resized image.\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1, 3, 1).title.set_text('original')\n",
    "    plt.imshow(original_img)\n",
    "    plt.subplot(1, 3, 2).title.set_text('input to model')\n",
    "    plt.imshow(convd_img)\n",
    "    plt.subplot(1, 3, 3).title.set_text('output from model')\n",
    "    output = run_ed_model(convd_img)\n",
    "    plt.imshow(output)\n",
    "    plt.show()\n",
    "    latent = run_encoder(convd_img)\n",
    "    print(f'Latent space: {latent}')\n",
    "    return latent\n",
    "\n",
    "l1 = show_images('https://i.redd.it/dchg4s8j485z.png')\n",
    "print('As the encoder wasn\\'t trained on a dataset of illustrations, it didn\\'t get to see many, if any images with solid backgrounds. We can expect this to improve with a larger latent space.')\n",
    "l2 = show_images('https://safebooru.org//samples/2886/sample_69950b7fd527da6e598cb65c244621e0010641af.jpg?3006072')\n",
    "print(f'Distance: {np.linalg.norm(np.asarray(l1)-np.asarray(l2))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of results with output shape (None, 4) and an unrelated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see if you set the input shape to (32, 32) and the output to (None, 4) trained on the cifar100 dataset, the results appear kinda awful. Or, That's what you would think.\n",
    "\n",
    "However, with output shape of only 4 floating point numbers, this isn't actually that bad. In fact, I'd say that it's about as good as you can get. It more just for me, showed that you can optimize the model for what you want it to accomplish.\n",
    "\n",
    "As they say, storage is cheap, but if you care about having a smaller hashes, you can use less principle components. This was a test, because phash is 64 bits long. Four floating point numbers, 128 bits is not much at all to compress an image down to.\n",
    "\n",
    "The objective here is not to reconstruct the image. The objective is to find images that are close. And this might not quite get you there, but it might actually get you partway.\n",
    "\n",
    "In practical implementation of a near-duplicate matching database, you're going to consume more, or orders of magnitude more space storing images or filepath/url references to images. In that way, you could justify larger output shapes.\n",
    "\n",
    "We have reason to believe that a larger output space will improve the quality of the model. What's pictured above won't work. The big questions now is how do large do we want the output space, how large do we want the input space, how many convolutional layers should there be, and by how much does the model improve when we change these things.\n",
    "\n",
    "![image](32_to_4_result.png)\n",
    "<center><b>Latent space: [195.22295  159.0578   153.84795  104.884476]</b></center>\n",
    "\n",
    "<br><br>\n",
    "<center>The model was actually trained on images like the ones here:<center>\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fcorochann.com%2Fwp-content%2Fuploads%2F2017%2F04%2Fcifar10_plot_more.png&f=1&nofb=1\" width=500, height=500, alt=\"cifar100 real-world picture dataset examples\">\n",
    "<br><br>\n",
    "<center>We should expect some degree of improvement when trained on illustrations, especially on images with solid backgrounds.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_input_layer will branch automatically.\n",
    "combined_input_layer = Input(shape=i_shape, name='Full_Model_Input')\n",
    "noised_input = GaussianNoise(0.1, name='Noise')(combined_input_layer)\n",
    "\n",
    "# Compress input and noised input to latent space, and hold on to a reference.\n",
    "encoder = get_encoder()\n",
    "decoder = get_decoder()\n",
    "vec1 = encoder(combined_input_layer)\n",
    "vec2 = encoder(noised_input)\n",
    "\n",
    "# Decode the original image from the latent space\n",
    "unencoded_original = decoder(vec1)\n",
    "\n",
    "combined_model = Model( \\\n",
    "    inputs=combined_input_layer, \\\n",
    "    outputs=[vec1, vec2, combined_input_layer, unencoded_original])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss, Complile Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out how to define loss like this without requiring the model to \n",
    "# output the second latent space, the input, or the decoded image.\n",
    "\n",
    "# y_true is what is passed to the fit() method, which we aren't actually using.\n",
    "# Those quantities will be unknown, this is unsupervised.\n",
    "# y_pred is the output of the model.\n",
    "def total_loss(y_true, y_pred):\n",
    "    dist = (K.sqrt(K.sum(K.square(y_pred[0] - y_pred[1]), axis=-1)))\n",
    "    mse = (K.mean(K.square(y_pred[2] - y_pred[3]), axis=-1))\n",
    "    return (distance_weight*dist) + (mse_weight*mse) # Weights were defined at the beginning\n",
    "\n",
    "combined_model.compile(optimizer='adam', loss=total_loss)\n",
    "print(combined_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the spot where I was having issues.\n",
    "train_full = True\n",
    "if train_full:\n",
    "    combined_model.fit(x_train, None, \n",
    "                        validation_data=(x_test, x_test), \n",
    "                        validation_split=0.25, # This is redundant, for when I strip away the seperate validation set. \n",
    "                        epochs=epoch_num, batch_size=batch_size, shuffle=True, \n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "# Indeed, it is a shape problem. Specidically, with the validation data. It's supposed to match up with\n",
    "# the shape of the output. However, I figure that there must be some way to train on things without having\n",
    "# to also output them. The only \"output\" that I'm really interested in using is vec1. Although I'll be stripping\n",
    "# away everything but the encoder, I feel like it would be good to know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
