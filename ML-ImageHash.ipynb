{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Compose, RandomCrop, RandomHorizontalFlip, RandomGrayscale, ToTensor, ToPILImage\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "from math import ceil\n",
    "import time\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Models import ResnetHasher, CustomHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "class ShowTensor():\n",
    "    def __call__(self, tensor):\n",
    "        npimg = tensor.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "show_tensor = ShowTensor()\n",
    "class ShowTriplet():\n",
    "    def __call__(self, triplet, axes=True):\n",
    "        npimgs = [ tensor.numpy() for tensor in triplet ] \n",
    "        fig, axs = plt.subplots(1,3, constrained_layout=True, figsize=(12,12))\n",
    "        axs[0].imshow(np.transpose(npimgs[0], (1, 2, 0)), interpolation='nearest')\n",
    "        axs[1].imshow(np.transpose(npimgs[1], (1, 2, 0)), interpolation='nearest')\n",
    "        axs[2].imshow(np.transpose(npimgs[2], (1, 2, 0)), interpolation='nearest')\n",
    "        axs[0].set_title(\"Anchor (A)\")\n",
    "        axs[1].set_title(\"Positive (P)\")\n",
    "        axs[2].set_title(\"Negative (N)\")\n",
    "        if not axes:\n",
    "            axs[0].set_axis_off()\n",
    "            axs[1].set_axis_off()\n",
    "            axs[2].set_axis_off()\n",
    "show_triplet = ShowTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize Model\n",
    "from torchsummary import summary\n",
    "def visualizeModel(model):\n",
    "    summary(model, (3, 64, 64))\n",
    "from Models import HasherBlock\n",
    "\n",
    "#print([x for x in ResnetHasher().resnet.children()])\n",
    "#ResnetHasher().visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomHasher().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation/Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RandomR4:\n",
    "    \"\"\"Rotate by one of the given angles.\"\"\"\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(x, angle)\n",
    "class ToRGBTensor:\n",
    "    def __init__(self):\n",
    "        self.tt = ToTensor()\n",
    "    def __call__(self, img):\n",
    "        return self.tt(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = True\n",
    "transforms = Compose([\n",
    "    RandomCrop((64, 64), padding=4),\n",
    "    RandomHorizontalFlip(.5),\n",
    "    RandomR4((0, 90, 180, 270)),\n",
    "    RandomGrayscale(0.3)\n",
    "]) if transform else Compose([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, directory, transforms, batch_size=64):\n",
    "        self.transforms = transforms\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.file_list = glob.glob(f'{directory}/*.png')\n",
    "        self.triplets_original = len(self.file_list) // 2\n",
    "        \n",
    "        self.tt = ToRGBTensor()\n",
    "        \n",
    "        print(f'Found {len(self.file_list)} images.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triplets_original\n",
    "\n",
    "    # (A, P, N)\n",
    "    def __getitem__(self, index):\n",
    "        anchor = self.file_list[index]\n",
    "        negative = self.file_list[self.triplets_original + index]\n",
    "        \n",
    "        A = Image.open(anchor)\n",
    "        P = self.transforms(A)\n",
    "        N = Image.open(negative)\n",
    "        \n",
    "        return (self.tt(A), self.tt(P), self.tt(N))\n",
    "    \n",
    "    def generate_batches(self):\n",
    "        triplets_retrieved = 0\n",
    "        while True:\n",
    "            tr_after_yield = triplets_retrieved + self.batch_size\n",
    "            \n",
    "            triplet_tensors = [ self.__getitem__(x) for x in range(triplets_retrieved, min(tr_after_yield, len(self))) ]\n",
    "            As=[];Ps=[];Ns=[]\n",
    "            for triplet in triplet_tensors:\n",
    "                As.append(triplet[0])\n",
    "                Ps.append(triplet[1])\n",
    "                Ns.append(triplet[2])\n",
    "            combined = As + Ps + Ns\n",
    "            \n",
    "            tensor_stack = torch.stack(combined)\n",
    "            yield tensor_stack\n",
    "            triplets_retrieved = tr_after_yield\n",
    "            \n",
    "            if triplets_retrieved >= self.triplets_original:\n",
    "                return\n",
    "    def len_batches(self):\n",
    "        return ceil(self.triplets_original / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = TripletDataset('Dataset', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "triplet = dataset[0]\n",
    "show_triplet(triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss:\n",
    "$$L(\\displaystyle A,\\displaystyle P,\\displaystyle N) = max\\Big( d(f(\\displaystyle A), f(\\displaystyle P)) − d(f(\\displaystyle A), f(\\displaystyle N)) + margin, 0 \\Big)$$\n",
    "\n",
    "Where:\n",
    "* ${\\displaystyle A}$ is an anchor input, \n",
    "* ${\\displaystyle P}$ is a positive input of the same class as ${\\displaystyle A}$, \n",
    "* ${\\displaystyle N}$ is a negative input of a different class from ${\\displaystyle A}$, \n",
    "* ${\\displaystyle \\alpha }$  is a margin between positive and negative pairs, \n",
    "* ${\\displaystyle \\operatorname{f}}$ is an embedding on a metric space, and \n",
    "* ${\\displaystyle \\operatorname{d}}$ is a distance function on that space (In this case L1 or L2 norm, Manhattan and Euclidean distance respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = CustomHasher()\n",
    "\n",
    "# Loss\n",
    "norm_margin = .2\n",
    "norm_type = 'Manhattan'\n",
    "loss_fn = nn.TripletMarginLoss(margin=norm_margin, p= 1 if norm_type=='Manhattan' else 2)\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_avg_losses = []\n",
    "\n",
    "# Train for 5 epochs\n",
    "for epoch in range(1, 100):\n",
    "    \n",
    "    batch_num = 0\n",
    "    batches = dataset.generate_batches()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    epoch_start_time = time.time()\n",
    "    for batch in batches:\n",
    "        # Batch is all As, then all Ps, then all Ns. There are always an equal number of each.\n",
    "        # Start batch timer\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Forward\n",
    "        model.zero_grad()\n",
    "        out_tensors = model.forward(batch)\n",
    "        A, P, N = out_tensors.split(len(batch) // 3)\n",
    "        \n",
    "        # Backward\n",
    "        loss = loss_fn(A, P, N)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        batch_end_time = time.time()\n",
    "        \n",
    "        \n",
    "        batch_num += 1\n",
    "        loss_num = loss.item()\n",
    "        epoch_losses.append(loss_num)\n",
    "        batch_time = round(batch_end_time-batch_start_time, 2)\n",
    "        len_batches = dataset.len_batches()\n",
    "        est_epoch_time_seconds = batch_time*len_batches\n",
    "        est_epoch_time_minutes = round(est_epoch_time_seconds / 60, 2)\n",
    "        print(f'Completed batch: {str(batch_num).rjust(4, \" \")} of {len_batches} | Loss: {str(round(loss_num, 2)).rjust(10, \" \")} | Time: {str(batch_time).rjust(6, \" \")} (est. {est_epoch_time_minutes} min for epoch)')\n",
    "        \n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time_minutes = round((epoch_end_time - epoch_start_time) / 60, 2)\n",
    "    epoch_average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    print(f\"\"\"\\\n",
    "╔═════════════════════════════════════════════════════════════╗\\n║ \\\n",
    "End of Epoch: {str(epoch).rjust(3, \" \")} | \\\n",
    "Average Loss: {str(round(epoch_average_loss, 2)).rjust(10, \" \")} | \\\n",
    "Time: {str(epoch_time_minutes).rjust(6, \" \")} ║\\n\\\n",
    "╚═════════════════════════════════════════════════════════════╝\"\"\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if (epoch > 5):\n",
    "        if (epoch_average_loss > epoch_average_loss[-1:]):\n",
    "            break\n",
    "    epoch_average_losses.append(epoch_average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
