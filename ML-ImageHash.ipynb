{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import Compose, RandomCrop, RandomHorizontalFlip, RandomGrayscale, ToTensor, ToPILImage\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.util import random_noise\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "from math import ceil\n",
    "import time\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Models import CustomHasher, SmallHasher, ResnetHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize Data\n",
    "class ShowTensor():\n",
    "    def __call__(self, tensor):\n",
    "        npimg = tensor.numpy()\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "show_tensor = ShowTensor()\n",
    "class ShowTriplet():\n",
    "    def __call__(self, triplet, axes=True):\n",
    "        npimgs = [ tensor.numpy() for tensor in triplet ] \n",
    "        fig, axs = plt.subplots(1,3, constrained_layout=True, figsize=(12,12))\n",
    "        axs[0].imshow(np.transpose(npimgs[0], (1, 2, 0)), interpolation='nearest')\n",
    "        axs[1].imshow(np.transpose(npimgs[1], (1, 2, 0)), interpolation='nearest')\n",
    "        axs[2].imshow(np.transpose(npimgs[2], (1, 2, 0)), interpolation='nearest')\n",
    "        axs[0].set_title(\"Anchor (A)\")\n",
    "        axs[1].set_title(\"Positive (P)\")\n",
    "        axs[2].set_title(\"Negative (N)\")\n",
    "        if not axes:\n",
    "            axs[0].set_axis_off()\n",
    "            axs[1].set_axis_off()\n",
    "            axs[2].set_axis_off()\n",
    "        plt.show()\n",
    "show_triplet = ShowTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1            [-1, 6, 64, 64]             168\n         MaxPool2d-2            [-1, 6, 32, 32]               0\n              ReLU-3            [-1, 6, 32, 32]               0\n       BatchNorm2d-4            [-1, 6, 32, 32]              12\n       HasherBlock-5            [-1, 6, 32, 32]               0\n            Conv2d-6           [-1, 12, 32, 32]             660\n         MaxPool2d-7           [-1, 12, 16, 16]               0\n              ReLU-8           [-1, 12, 16, 16]               0\n       BatchNorm2d-9           [-1, 12, 16, 16]              24\n      HasherBlock-10           [-1, 12, 16, 16]               0\n           Conv2d-11           [-1, 24, 16, 16]           2,616\n        MaxPool2d-12             [-1, 24, 8, 8]               0\n             ReLU-13             [-1, 24, 8, 8]               0\n      BatchNorm2d-14             [-1, 24, 8, 8]              48\n      HasherBlock-15             [-1, 24, 8, 8]               0\n           Conv2d-16             [-1, 48, 8, 8]          10,416\n        MaxPool2d-17             [-1, 48, 4, 4]               0\n             ReLU-18             [-1, 48, 4, 4]               0\n      BatchNorm2d-19             [-1, 48, 4, 4]              96\n      HasherBlock-20             [-1, 48, 4, 4]               0\n           Conv2d-21             [-1, 48, 2, 2]          20,784\n        MaxPool2d-22             [-1, 48, 1, 1]               0\n             ReLU-23             [-1, 48, 1, 1]               0\n      BatchNorm2d-24             [-1, 48, 1, 1]              96\n      HasherBlock-25             [-1, 48, 1, 1]               0\nAdaptiveMaxPool2d-26             [-1, 48, 1, 1]               0\n          Flatten-27                   [-1, 48]               0\n           Linear-28                   [-1, 20]             980\n           Linear-29                    [-1, 8]             168\n             Tanh-30                    [-1, 8]               0\n================================================================\nTotal params: 36,068\nTrainable params: 36,068\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.71\nParams size (MB): 0.14\nEstimated Total Size (MB): 0.89\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "# Visualize Model\n",
    "SmallHasher().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation/Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class RandomRotate:\n",
    "    # Rotate by one of the given angles.\n",
    "    def __init__(self, angles):\n",
    "        self.angles = angles\n",
    "    def __call__(self, x):\n",
    "        angle = random.choice(self.angles)\n",
    "        return TF.rotate(x, angle)\n",
    "\n",
    "class RandomFillCrop:\n",
    "    def __init__(self, chance):\n",
    "        self.chance = chance\n",
    "        self.rcs = [ RandomCrop((64, 64), padding=4, fill=0), \n",
    "                     RandomCrop((64, 64), padding=4, fill=255) ] \n",
    "    def __call__(self, img):\n",
    "        return random.choice(self.rcs)(img) if random.random() < self.chance else img\n",
    "\n",
    "class Noise:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, img):\n",
    "        noise_type = random.random()\n",
    "        if noise_type < .3:\n",
    "            noise_type = 's&p'\n",
    "        elif noise_type < .6:\n",
    "            noise_type = 'gaussian'\n",
    "        elif noise_type < .9:\n",
    "            noise_type = 'speckle'\n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "        return Image.fromarray(random_noise(np.asarray(img), mode=noise_type))\n",
    "\n",
    "# Used in model\n",
    "class ToRGBTensor:\n",
    "    def __init__(self):\n",
    "        self.tt = ToTensor()\n",
    "    def __call__(self, img):\n",
    "        return self.tt(img.convert('RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "transforms = Compose([\n",
    "    RandomFillCrop(.8),\n",
    "    Noise(),\n",
    "    RandomHorizontalFlip(.5),\n",
    "    RandomRotate(range(0, 360, 30)),\n",
    "    RandomGrayscale(0.3)\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, directory, transforms, batch_size=64):\n",
    "        self.transforms = transforms\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.file_list = glob.glob(f'{directory}/*.png')\n",
    "        self.triplets_original = len(self.file_list) // 2\n",
    "        \n",
    "        self.tt = ToRGBTensor()\n",
    "        \n",
    "        print(f'Found {len(self.file_list)} images.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.triplets_original\n",
    "\n",
    "    # (A, P, N)\n",
    "    def __getitem__(self, index):\n",
    "        anchor = self.file_list[index]\n",
    "        negative = self.file_list[self.triplets_original + index]\n",
    "        \n",
    "        A = Image.open(anchor)\n",
    "        P = self.transforms(A)\n",
    "        N = Image.open(negative)\n",
    "        \n",
    "        return (self.tt(A), self.tt(P), self.tt(N))\n",
    "    \n",
    "    def generate_batches(self):\n",
    "        triplets_retrieved = 0\n",
    "        while True:\n",
    "            tr_after_yield = triplets_retrieved + self.batch_size\n",
    "            \n",
    "            triplet_tensors = [ self.__getitem__(x) for x in range(triplets_retrieved, min(tr_after_yield, len(self))) ]\n",
    "            As=[];Ps=[];Ns=[]\n",
    "            for triplet in triplet_tensors:\n",
    "                As.append(triplet[0])\n",
    "                Ps.append(triplet[1])\n",
    "                Ns.append(triplet[2])\n",
    "            combined = As + Ps + Ns\n",
    "            \n",
    "            tensor_stack = torch.stack(combined)\n",
    "            yield tensor_stack\n",
    "            triplets_retrieved = tr_after_yield\n",
    "            \n",
    "            if triplets_retrieved >= self.triplets_original:\n",
    "                return\n",
    "    def num_batches(self):\n",
    "        return ceil(self.triplets_original / self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 537379 images.\n"
    }
   ],
   "source": [
    "dataset = TripletDataset('TrainDataset', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 179127 images.\n"
    }
   ],
   "source": [
    "validationset = TripletDataset('ValidDataset', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 4), <f8",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 4), '<f8')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6545d659d7a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtriplet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_triplet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtriplet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidationset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_triplet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-df409bdf315b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f235c4a53169>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Used in model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2680\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 4), <f8"
     ]
    }
   ],
   "source": [
    "triplet = dataset[0]\n",
    "show_triplet(triplet)\n",
    "triplet = validationset[0]\n",
    "show_triplet(triplet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss:\n",
    "$$L(\\displaystyle A,\\displaystyle P,\\displaystyle N) = max\\Big( d(f(\\displaystyle A), f(\\displaystyle P)) − d(f(\\displaystyle A), f(\\displaystyle N)) + margin, 0 \\Big)$$\n",
    "\n",
    "Where:\n",
    "* ${\\displaystyle A}$ is an anchor input, \n",
    "* ${\\displaystyle P}$ is a positive input of the same class as ${\\displaystyle A}$, \n",
    "* ${\\displaystyle N}$ is a negative input of a different class from ${\\displaystyle A}$, \n",
    "* ${\\displaystyle \\alpha }$  is a margin between positive and negative pairs, \n",
    "* ${\\displaystyle \\operatorname{f}}$ is an embedding on a metric space, and \n",
    "* ${\\displaystyle \\operatorname{d}}$ is a distance function on that space (In this case L1 or L2 norm, Manhattan and Euclidean distance respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = CustomHasher()\n",
    "\n",
    "# Loss\n",
    "norm_margin = .4\n",
    "norm_type = 'Manhattan'\n",
    "loss_fn = nn.TripletMarginLoss(margin=norm_margin, p= 1 if norm_type=='Manhattan' else 2)\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<fstring>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<fstring>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    (\"{:6.4f}\".format(loss_num)))\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "print_batches = True\n",
    "\n",
    "# Returns loss\n",
    "def train_batch(model, batch, batchnum):\n",
    "        batch_start_time = time.time()\n",
    "        \n",
    "        # Forward\n",
    "        model.zero_grad()\n",
    "        out_tensors = model.forward(batch)\n",
    "        A, P, N = out_tensors.split(len(batch) // 3)\n",
    "        \n",
    "        # Backward\n",
    "        loss = loss_fn(A, P, N)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        batch_end_time = time.time()\n",
    "        \n",
    "        if print_batches:\n",
    "            loss_num = loss.item()\n",
    "            batch_time = round(batch_end_time-batch_start_time, 2)\n",
    "            num_batches = dataset.num_batches()\n",
    "            est_epoch_time_seconds = batch_time*num_batches\n",
    "            est_epoch_time_minutes = round(est_epoch_time_seconds / 60, 2)\n",
    "            print(f'Completed train batch: {str(batch_num).rjust(4, \" \")} of {num_batches} | \\\n",
    "Loss: {\"{:6.4f}\".format(loss_num))} | \\\n",
    "Time: {str(batch_time).rjust(6, \" \")} (est. {est_epoch_time_minutes} min for epoch)')\n",
    "        return loss_num\n",
    "\n",
    "# Returns loss\n",
    "def valid_batch(model, batch, batch_num):\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Forward\n",
    "        model.zero_grad()\n",
    "        out_tensors = model.forward(vbatch)\n",
    "        A, P, N = out_tensors.split(len(vbatch) // 3)\n",
    "        \n",
    "        # Backward\n",
    "        loss = loss_fn(A, P, N)\n",
    "        batch_end_time = time.time()\n",
    "\n",
    "        loss_num = loss.item()\n",
    "        \n",
    "        if print_batches:\n",
    "            batch_time = round(batch_end_time-batch_start_time, 2)\n",
    "            num_batches = validationset.num_batches()\n",
    "            est_epoch_time_seconds = batch_time*num_batches\n",
    "            est_epoch_time_minutes = round(est_epoch_time_seconds / 60, 2)\n",
    "            print(f'Completed valid batch: {str(batch_num).rjust(4, \" \")} of {num_batches} | \\\n",
    "Loss: {\"{:6.4f}\".format(loss_num)} | \\\n",
    "Time: {str(batch_time).rjust(6, \" \")} (est. {est_epoch_time_minutes} min for epoch)')\n",
    "        return loss_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 4), <f8",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2680\u001b[0;31m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 4), '<f8')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-67360c8a8974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Training Batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-df409bdf315b>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtr_after_yield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplets_retrieved\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtriplet_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets_retrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_after_yield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mAs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mPs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mNs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtriplet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtriplet_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-df409bdf315b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mtr_after_yield\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriplets_retrieved\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mtriplet_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplets_retrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_after_yield\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mAs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mPs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mNs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtriplet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtriplet_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-df409bdf315b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f235c4a53169>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Used in model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2680\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fromarray_typemap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtypekey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtypekey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 4), <f8"
     ]
    }
   ],
   "source": [
    "epoch_avg_losses = []\n",
    "# Train for 5 epochs\n",
    "for epoch in range(1, 100):\n",
    "    \n",
    "    batch_num = 0\n",
    "    batch_valid_losses = []\n",
    "    \n",
    "    # Training Batches\n",
    "    epoch_start_time = time.time()\n",
    "    for batch in dataset.generate_batches():\n",
    "        batch_num += 1\n",
    "        \n",
    "        train_batch(model, batch, batch_num)\n",
    "\n",
    "    # Validation batches\n",
    "    batch_num = 0\n",
    "    for vbatch in validationset.generate_batches():\n",
    "        batch_num += 1\n",
    "\n",
    "        loss_num = valid_batch(model, vbatch, batch_num)\n",
    "        batch_valid_losses.append(loss_num)\n",
    "    \n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time_minutes = round((epoch_end_time - epoch_start_time) / 60, 2)\n",
    "    epoch_average_train_loss = sum(batch_valid_losses) / len(batch_valid_losses)\n",
    "    print(f\"\"\"\\\n",
    "╔══════════════════════════════════════════════════════════════════╗\\n║ \\\n",
    "End of Epoch: {str(epoch).rjust(3, \" \")} | \\\n",
    "Average Loss: {str(round(epoch_average_train_loss, 2)).rjust(10, \" \")} | \\\n",
    "Time: {str(epoch_time_minutes).rjust(6, \" \")} min. ║\\n\\\n",
    "╚══════════════════════════════════════════════════════════════════╝\"\"\")\n",
    "    \n",
    "    # Early Stopping\n",
    "    if (epoch > 5):\n",
    "        if (epoch_average_train_loss > epoch_avg_losses[-1]):\n",
    "            break\n",
    "    epoch_avg_losses.append(epoch_average_train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}