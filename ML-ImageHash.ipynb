{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/apaz/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "loss_margin = .2\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        class OntoF16(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(OntoF16, self).__init__()\n",
    "                \n",
    "                self.f16_map = 2**12 - 1\n",
    "            def forward(self, x):\n",
    "                return x * self.f16_map\n",
    "        \n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "        self.enc_layers = OrderedDict([ \n",
    "            ('resnet', self.resnet),\n",
    "                                       \n",
    "            ('full_1', nn.Linear(1000, 750)), \n",
    "            ('relu_1', nn.ReLU()),\n",
    "            ('norm_1', nn.BatchNorm1d(750)),\n",
    "            \n",
    "            ('full_2', nn.Linear(750, 500)),\n",
    "            ('relu_2', nn.ReLU()),\n",
    "            ('norm_2', nn.BatchNorm1d(500)),\n",
    "            \n",
    "            ('full_3', nn.Linear(500, 250)),\n",
    "            ('relu_3', nn.ReLU()),\n",
    "            ('norm_3', nn.BatchNorm1d(250)),\n",
    "            \n",
    "            ('output', nn.Linear(250, 8)),\n",
    "            ('sigmoid', nn.Sigmoid()),\n",
    "            ('ontoF16', OntoF16())\n",
    "        ])\n",
    "        self.encoder = nn.Sequential(self.enc_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/apaz/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           9,408\n",
      "            Conv2d-2           [-1, 64, 32, 32]           9,408\n",
      "       BatchNorm2d-3           [-1, 64, 32, 32]             128\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "              ReLU-6           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-7           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-8           [-1, 64, 16, 16]               0\n",
      "            Conv2d-9           [-1, 64, 16, 16]          36,864\n",
      "           Conv2d-10           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 16, 16]             128\n",
      "      BatchNorm2d-12           [-1, 64, 16, 16]             128\n",
      "             ReLU-13           [-1, 64, 16, 16]               0\n",
      "             ReLU-14           [-1, 64, 16, 16]               0\n",
      "           Conv2d-15           [-1, 64, 16, 16]          36,864\n",
      "           Conv2d-16           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 16, 16]             128\n",
      "      BatchNorm2d-18           [-1, 64, 16, 16]             128\n",
      "             ReLU-19           [-1, 64, 16, 16]               0\n",
      "             ReLU-20           [-1, 64, 16, 16]               0\n",
      "       BasicBlock-21           [-1, 64, 16, 16]               0\n",
      "       BasicBlock-22           [-1, 64, 16, 16]               0\n",
      "           Conv2d-23           [-1, 64, 16, 16]          36,864\n",
      "           Conv2d-24           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-25           [-1, 64, 16, 16]             128\n",
      "      BatchNorm2d-26           [-1, 64, 16, 16]             128\n",
      "             ReLU-27           [-1, 64, 16, 16]               0\n",
      "             ReLU-28           [-1, 64, 16, 16]               0\n",
      "           Conv2d-29           [-1, 64, 16, 16]          36,864\n",
      "           Conv2d-30           [-1, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 16, 16]             128\n",
      "      BatchNorm2d-32           [-1, 64, 16, 16]             128\n",
      "             ReLU-33           [-1, 64, 16, 16]               0\n",
      "             ReLU-34           [-1, 64, 16, 16]               0\n",
      "       BasicBlock-35           [-1, 64, 16, 16]               0\n",
      "       BasicBlock-36           [-1, 64, 16, 16]               0\n",
      "           Conv2d-37            [-1, 128, 8, 8]          73,728\n",
      "           Conv2d-38            [-1, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-39            [-1, 128, 8, 8]             256\n",
      "      BatchNorm2d-40            [-1, 128, 8, 8]             256\n",
      "             ReLU-41            [-1, 128, 8, 8]               0\n",
      "             ReLU-42            [-1, 128, 8, 8]               0\n",
      "           Conv2d-43            [-1, 128, 8, 8]         147,456\n",
      "           Conv2d-44            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-45            [-1, 128, 8, 8]             256\n",
      "      BatchNorm2d-46            [-1, 128, 8, 8]             256\n",
      "           Conv2d-47            [-1, 128, 8, 8]           8,192\n",
      "           Conv2d-48            [-1, 128, 8, 8]           8,192\n",
      "      BatchNorm2d-49            [-1, 128, 8, 8]             256\n",
      "      BatchNorm2d-50            [-1, 128, 8, 8]             256\n",
      "             ReLU-51            [-1, 128, 8, 8]               0\n",
      "             ReLU-52            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-53            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-54            [-1, 128, 8, 8]               0\n",
      "           Conv2d-55            [-1, 128, 8, 8]         147,456\n",
      "           Conv2d-56            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-57            [-1, 128, 8, 8]             256\n",
      "      BatchNorm2d-58            [-1, 128, 8, 8]             256\n",
      "             ReLU-59            [-1, 128, 8, 8]               0\n",
      "             ReLU-60            [-1, 128, 8, 8]               0\n",
      "           Conv2d-61            [-1, 128, 8, 8]         147,456\n",
      "           Conv2d-62            [-1, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-63            [-1, 128, 8, 8]             256\n",
      "      BatchNorm2d-64            [-1, 128, 8, 8]             256\n",
      "             ReLU-65            [-1, 128, 8, 8]               0\n",
      "             ReLU-66            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-67            [-1, 128, 8, 8]               0\n",
      "       BasicBlock-68            [-1, 128, 8, 8]               0\n",
      "           Conv2d-69            [-1, 256, 4, 4]         294,912\n",
      "           Conv2d-70            [-1, 256, 4, 4]         294,912\n",
      "      BatchNorm2d-71            [-1, 256, 4, 4]             512\n",
      "      BatchNorm2d-72            [-1, 256, 4, 4]             512\n",
      "             ReLU-73            [-1, 256, 4, 4]               0\n",
      "             ReLU-74            [-1, 256, 4, 4]               0\n",
      "           Conv2d-75            [-1, 256, 4, 4]         589,824\n",
      "           Conv2d-76            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-77            [-1, 256, 4, 4]             512\n",
      "      BatchNorm2d-78            [-1, 256, 4, 4]             512\n",
      "           Conv2d-79            [-1, 256, 4, 4]          32,768\n",
      "           Conv2d-80            [-1, 256, 4, 4]          32,768\n",
      "      BatchNorm2d-81            [-1, 256, 4, 4]             512\n",
      "      BatchNorm2d-82            [-1, 256, 4, 4]             512\n",
      "             ReLU-83            [-1, 256, 4, 4]               0\n",
      "             ReLU-84            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-85            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-86            [-1, 256, 4, 4]               0\n",
      "           Conv2d-87            [-1, 256, 4, 4]         589,824\n",
      "           Conv2d-88            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-89            [-1, 256, 4, 4]             512\n",
      "      BatchNorm2d-90            [-1, 256, 4, 4]             512\n",
      "             ReLU-91            [-1, 256, 4, 4]               0\n",
      "             ReLU-92            [-1, 256, 4, 4]               0\n",
      "           Conv2d-93            [-1, 256, 4, 4]         589,824\n",
      "           Conv2d-94            [-1, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-95            [-1, 256, 4, 4]             512\n",
      "      BatchNorm2d-96            [-1, 256, 4, 4]             512\n",
      "             ReLU-97            [-1, 256, 4, 4]               0\n",
      "             ReLU-98            [-1, 256, 4, 4]               0\n",
      "       BasicBlock-99            [-1, 256, 4, 4]               0\n",
      "      BasicBlock-100            [-1, 256, 4, 4]               0\n",
      "          Conv2d-101            [-1, 512, 2, 2]       1,179,648\n",
      "          Conv2d-102            [-1, 512, 2, 2]       1,179,648\n",
      "     BatchNorm2d-103            [-1, 512, 2, 2]           1,024\n",
      "     BatchNorm2d-104            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-105            [-1, 512, 2, 2]               0\n",
      "            ReLU-106            [-1, 512, 2, 2]               0\n",
      "          Conv2d-107            [-1, 512, 2, 2]       2,359,296\n",
      "          Conv2d-108            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-109            [-1, 512, 2, 2]           1,024\n",
      "     BatchNorm2d-110            [-1, 512, 2, 2]           1,024\n",
      "          Conv2d-111            [-1, 512, 2, 2]         131,072\n",
      "          Conv2d-112            [-1, 512, 2, 2]         131,072\n",
      "     BatchNorm2d-113            [-1, 512, 2, 2]           1,024\n",
      "     BatchNorm2d-114            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-115            [-1, 512, 2, 2]               0\n",
      "            ReLU-116            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-117            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-118            [-1, 512, 2, 2]               0\n",
      "          Conv2d-119            [-1, 512, 2, 2]       2,359,296\n",
      "          Conv2d-120            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-121            [-1, 512, 2, 2]           1,024\n",
      "     BatchNorm2d-122            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-123            [-1, 512, 2, 2]               0\n",
      "            ReLU-124            [-1, 512, 2, 2]               0\n",
      "          Conv2d-125            [-1, 512, 2, 2]       2,359,296\n",
      "          Conv2d-126            [-1, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-127            [-1, 512, 2, 2]           1,024\n",
      "     BatchNorm2d-128            [-1, 512, 2, 2]           1,024\n",
      "            ReLU-129            [-1, 512, 2, 2]               0\n",
      "            ReLU-130            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-131            [-1, 512, 2, 2]               0\n",
      "      BasicBlock-132            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-133            [-1, 512, 1, 1]               0\n",
      "AdaptiveAvgPool2d-134            [-1, 512, 1, 1]               0\n",
      "          Linear-135                 [-1, 1000]         513,000\n",
      "          Linear-136                 [-1, 1000]         513,000\n",
      "          ResNet-137                 [-1, 1000]               0\n",
      "          ResNet-138                 [-1, 1000]               0\n",
      "          Linear-139                  [-1, 750]         750,750\n",
      "            ReLU-140                  [-1, 750]               0\n",
      "     BatchNorm1d-141                  [-1, 750]           1,500\n",
      "          Linear-142                  [-1, 500]         375,500\n",
      "            ReLU-143                  [-1, 500]               0\n",
      "     BatchNorm1d-144                  [-1, 500]           1,000\n",
      "          Linear-145                  [-1, 250]         125,250\n",
      "            ReLU-146                  [-1, 250]               0\n",
      "     BatchNorm1d-147                  [-1, 250]             500\n",
      "          Linear-148                    [-1, 8]           2,008\n",
      "         Sigmoid-149                    [-1, 8]               0\n",
      "         OntoF16-150                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 24,635,532\n",
      "Trainable params: 24,635,532\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 10.32\n",
      "Params size (MB): 93.98\n",
      "Estimated Total Size (MB): 104.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "def visualize(model):\n",
    "    summary(model, (3, 64, 64))\n",
    "\n",
    "encoder = Encoder()\n",
    "visualize(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "![title](https://wikimedia.org/api/rest_v1/media/math/render/svg/933c19129ec9060b0e7ea6f54f715c4c92010399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(a, p, n) : \n",
    "    d = nn.PairwiseDistance(p=2)\n",
    "    distance = d(a, p) - d(a, n) + loss_margin \n",
    "    loss = torch.mean(torch.max(distance, torch.zeros_like(distance))) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
