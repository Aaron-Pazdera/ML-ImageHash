{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "from math import ceil\n",
    "import time\n",
    "import random\n",
    "import glob\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 32, 32]             448\n         MaxPool2d-2           [-1, 16, 32, 32]               0\n              ReLU-3           [-1, 16, 32, 32]               0\n       BatchNorm2d-4           [-1, 16, 32, 32]              32\n       HasherBlock-5           [-1, 16, 32, 32]               0\n            Conv2d-6           [-1, 20, 16, 16]           2,900\n         MaxPool2d-7           [-1, 20, 16, 16]               0\n              ReLU-8           [-1, 20, 16, 16]               0\n       BatchNorm2d-9           [-1, 20, 16, 16]              40\n      HasherBlock-10           [-1, 20, 16, 16]               0\n           Conv2d-11             [-1, 28, 8, 8]           5,068\n        MaxPool2d-12             [-1, 28, 8, 8]               0\n             ReLU-13             [-1, 28, 8, 8]               0\n      BatchNorm2d-14             [-1, 28, 8, 8]              56\n      HasherBlock-15             [-1, 28, 8, 8]               0\n           Conv2d-16             [-1, 32, 4, 4]           8,096\n        MaxPool2d-17             [-1, 32, 4, 4]               0\n             ReLU-18             [-1, 32, 4, 4]               0\n      BatchNorm2d-19             [-1, 32, 4, 4]              64\n      HasherBlock-20             [-1, 32, 4, 4]               0\n           Conv2d-21             [-1, 40, 2, 2]          11,560\n        MaxPool2d-22             [-1, 40, 2, 2]               0\n             ReLU-23             [-1, 40, 2, 2]               0\n      BatchNorm2d-24             [-1, 40, 2, 2]              80\n      HasherBlock-25             [-1, 40, 2, 2]               0\nAdaptiveMaxPool2d-26             [-1, 40, 1, 1]               0\n          Flatten-27                   [-1, 40]               0\n           Linear-28                   [-1, 24]             984\n             ReLU-29                   [-1, 24]               0\n           Linear-30                   [-1, 16]             400\n             Tanh-31                   [-1, 16]               0\n================================================================\nTotal params: 29,728\nTrainable params: 29,728\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.92\nParams size (MB): 0.11\nEstimated Total Size (MB): 1.08\n----------------------------------------------------------------\n"
    }
   ],
   "source": [
    "from Models import CustomHasher, SmallHasher, ResnetHasher\n",
    "SmallHasher().visualize()\n",
    "model = SmallHasher()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TripletDataset(torch.utils.data.Dataset):    \n",
    "    def __init__(self, directory, transforms, n=6):\n",
    "        self.transforms = transforms\n",
    "        self.n = n\n",
    "        \n",
    "        self.file_list = glob.glob(f'{directory}/*.png')\n",
    "        self.file_list.sort()\n",
    "        self.num_items_available = len(self.file_list) // self.n\n",
    "        self.base_idxes = [self.num_items_available * i for i in range(0, self.n)]\n",
    "\n",
    "        # from imageaug\n",
    "        self._tt = ToRGBTensor()\n",
    "        \n",
    "        print(f'Found {len(self.file_list)} images.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_items_available\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        names     = [self.file_list[pos + index] for pos in self.base_idxes]\n",
    "\n",
    "        anchors   = [Image.open(name) for name in names]\n",
    "        positives = [self.transforms(anchor) for anchor in anchors]\n",
    "        \n",
    "        anchors   = [self._tt(img) for img in anchors]\n",
    "        positives = [self._tt(img) for img in positives]\n",
    "\n",
    "        return (anchors, positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation/Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "from imageaug import *\n",
    "\n",
    "transforms = Compose([\n",
    "    ApplyOne(Noise(), Greyscale()),\n",
    "    ApplyOne(RandomRotate(range(0, 360, 30)), RandomFillCrop(1))\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 537379 images.\nFound 179127 images.\n"
    }
   ],
   "source": [
    "dataset = TripletDataset('TrainDataset', transforms)\n",
    "validationset = TripletDataset('ValidDataset', transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=64)\n",
    "valid_loader = DataLoader(validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vistools import show_ntuples\n",
    "\n",
    "show_ntuples(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Loss:\n",
    "$$L(\\displaystyle A,\\displaystyle P,\\displaystyle N,\\displaystyle \\alpha) = max\\Big( d(f(\\displaystyle A), f(\\displaystyle N)) − d(f(\\displaystyle A), f(\\displaystyle P)) + \\displaystyle \\alpha, 0 \\Big)$$\n",
    "\n",
    "Where:\n",
    "* ${\\displaystyle A}$ is an anchor input, \n",
    "* ${\\displaystyle P}$ is a positive input of the same class as ${\\displaystyle A}$, \n",
    "* ${\\displaystyle N}$ is a negative input of a different class from ${\\displaystyle A}$, \n",
    "* ${\\displaystyle \\alpha }$  is a margin between positive and negative pairs, \n",
    "* ${\\displaystyle f}$ is an embedding onto a metric space (the model we're training), and \n",
    "* $d$ is a distance function on that space. (In this case L1 or L2 norm, Manhattan and Euclidean distance respectively).\n",
    "\n",
    "Taken from FaceNet paper\n",
    "https://arxiv.org/abs/1503.03832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Tuple Loss\n",
    "Triplet loss as described above accomplishes unsupervised learning of embeddings. This is all well and good, but what if we wanted to up the ante a bit? Triplet loss pushes the embeddings of two images farther apart. But what if we wanted to push even more embeddings apart, probably more intelligently, in a single step? Behold my monstrosity, n-tuple loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, let $f$ be an embedding from the input space onto metric space, and $\\alpha$ be the desired margin between positive and negative pairs. In this case, $f$ represents our model.\n",
    "\n",
    "Let $A$ be a tuple of $n$ anchor inputs from separate classes, and $P$ be a tuple of $n$ corresponding positive matches from each class. \n",
    "\n",
    "Let the class of $a \\in A$ be distinct from the class of $x \\in A\\cup B$ for $a \\neq x$.\n",
    "\n",
    "Let $S$ be a tuple of distances on the chosen metric space of length n.\n",
    "Let $S_i = d(f(A_i), f(B_i))$ for $i \\in 1..n$\n",
    "\n",
    "Finally, \n",
    "\n",
    "$$L(\\displaystyle A,\\displaystyle P,\\displaystyle \\alpha) = \\sum_{i=0}^{n-1} \\sum_{j=i+1}^{n-1} \\lbrack d(f(A_i),f(P_j)) - S_j + \\alpha \\rbrack_{+}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration of Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'list'> of size 2 of\n<class 'list'> of size 6 of\n<class 'torch.Tensor'> of shape torch.Size([64, 3, 64, 64])\n\n<class 'tuple'> of 64\n<class 'torch.Tensor'> of shape torch.Size([6, 16])\n\n<class 'list'> of length 64\n<class 'torch.Tensor'> of shape torch.Size([6])\n"
    }
   ],
   "source": [
    "from torch.nn.functional import pairwise_distance\n",
    "\n",
    "dat = train_loader.__iter__().__next__()\n",
    "print(type(dat), 'of size', len(dat), 'of')\n",
    "print(type(dat[0]), 'of size', len(dat[0]), 'of')\n",
    "print(type(dat[0][0]), 'of shape', dat[0][0].shape)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    positives = model.forward(torch.stack(dat[0]).view(-1, 3, 64, 64))\n",
    "    negatives = model.forward(torch.stack(dat[1]).view(-1, 3, 64, 64))\n",
    "\n",
    "    positives = torch.chunk(positives.view(-1, 16), train_loader.batch_size)\n",
    "    negatives = torch.chunk(negatives.view(-1, 16), train_loader.batch_size)\n",
    "\n",
    "    print()\n",
    "    print(type(positives), 'of', len(positives))\n",
    "    print(type(positives[0]), 'of shape', positives[0].shape)\n",
    "\n",
    "S = [ pairwise_distance(p, n, p=2) for p, n in zip(positives, negatives) ]\n",
    "print()\n",
    "print(type(S), 'of length', len(S))\n",
    "print(type(S[0]), 'of shape', S[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Tuple Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import pairwise_distance\n",
    "\n",
    "def loss_fn(anchor_embeddings, positive_embeddings, α, p=2):\n",
    "    # Embeddings are tuples of batchnum many embedding tensor stacks. These stacks have shape (n, embedding_size).\n",
    "    S = [ pairwise_distance(an, pos, p=p) for an, pos in zip(anchor_embeddings, positive_embeddings) ]\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(0, n):\n",
    "        for j in range(i, n):\n",
    "            dist        = pairwise_distance(anchor_embeddings[i], positive_embeddings[j], p=p)\n",
    "            single_loss = torch.max(dist + alpha,)\n",
    "            losses.append(single_loss)\n",
    "            \n",
    "            \n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "model = SmallHasher()\n",
    "\n",
    "# Loss\n",
    "alpha = .4\n",
    "norm_type = 'Manhattan'\n",
    "\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that one batch actually contains So, each batch actually contains n * 2 * batchnum images each. They're arranged as such.\n",
    "* batch (list of length batch_num)\n",
    "* ntuples (list of length 2)\n",
    "* images (list of length n)\n",
    "* image (tensor of shape 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns loss\n",
    "def train_batch(model, batch, batchnum):\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Enable Training\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        anchors, positives = torch.stack(batch[0]), torch.stack(batch[1])\n",
    "\n",
    "        anchor_embeddings   = model.forward(anchors)\n",
    "        positive_embeddings = model.forward(positives)\n",
    "        \n",
    "        # Backward\n",
    "        loss = loss_fn(anchor_embeddings, positive_embeddings, dataset.n)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Print\n",
    "        batch_end_time = time.time()\n",
    "        loss_num = loss.item()\n",
    "        if print_batches:\n",
    "            print_batch('train', loss_num, dataset, batch_start_time, batch_end_time)\n",
    "        \n",
    "        return loss_num\n",
    "\n",
    "# Returns loss\n",
    "def valid_batch(model, batch, batch_num):\n",
    "        batch_start_time = time.time()\n",
    "\n",
    "        # Disable Training\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "                # Forward\n",
    "                model.zero_grad()\n",
    "                out_tensors = model.forward(vbatch)\n",
    "                A = out_tensors[0::3]\n",
    "                P = out_tensors[1::3]\n",
    "                N = out_tensors[2::3]\n",
    "        \n",
    "                # Backward\n",
    "                loss = loss_fn(A, P, N)\n",
    "                batch_end_time = time.time()\n",
    "\n",
    "                loss_num = loss.item()\n",
    "        \n",
    "        if print_batches:\n",
    "            print_batch('valid', loss_num, dataset, batch_start_time, batch_end_time)\n",
    "\n",
    "        return loss_num\n",
    "\n",
    "# Returns new best weights\n",
    "def save(model, best_weights, best_loss, loss_num):\n",
    "        if best_loss <= loss_num:\n",
    "                return best_weights, best_loss\n",
    "        return copy.deepcopy(model.state_dict()), loss_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_batches = True\n",
    "def print_batch(tv, loss_num, dataset, batch_start_time, batch_end_time):\n",
    "        batch_time = round(batch_end_time-batch_start_time, 2)\n",
    "        num_batches = dataset.num_batches()\n",
    "        est_epoch_time_seconds = batch_time*num_batches\n",
    "        est_epoch_time_minutes = round(est_epoch_time_seconds / 60, 2)\n",
    "        print(f'\\rCompleted {tv} batch: {str(batch_num).rjust(4, \" \")} of {num_batches} | \\\n",
    "Loss: {\"{:6.4f}\".format(loss_num)} | \\\n",
    "Time: {str(batch_time).rjust(6, \" \")} (est. {\"{:4.1f}\".format(est_epoch_time_minutes)} min for epoch)', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "conv2d(): argument 'input' (position 1) must be Tensor, not list",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ba5e0f39ecab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mbatch_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# So carriage return prints train/valid on different lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-38c15f3cf930>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(model, batch, batchnum)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0manchor_embeddings\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mpositive_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/git/ML-ImageHash/Models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/git/ML-ImageHash/Models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/apaz/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "best_weights = copy.deepcopy(model.state_dict())\n",
    "best_loss = np.inf\n",
    "\n",
    "\n",
    "epoch_avg_valid_losses = []\n",
    "\n",
    "# Train for at most 1000 epochs. There's no way it will take that long.\n",
    "for epoch in range(1, 1001):\n",
    "    \n",
    "    batch_num = 0\n",
    "    batch_valid_losses = []\n",
    "    \n",
    "    # Training Batches\n",
    "    epoch_start_time = time.time()\n",
    "    for batch_num, batch in enumerate(train_loader):\n",
    "        \n",
    "        train_batch(model, batch, batch_num)\n",
    "    \n",
    "    # So carriage return prints train/valid on different lines\n",
    "    if print_batches:\n",
    "        print()\n",
    "\n",
    "    # Validation batches\n",
    "    batch_num = 0\n",
    "    for batch_num, vbatch in enumerate(valid_loader):\n",
    "        \n",
    "\n",
    "        loss_num = valid_batch(model, vbatch, batch_num)\n",
    "        batch_valid_losses.append(loss_num)\n",
    "    print()\n",
    "\n",
    "    # Print epoch results\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time_minutes = round((epoch_end_time - epoch_start_time) / 60, 2)\n",
    "    epoch_average_valid_loss = sum(batch_valid_losses) / len(batch_valid_losses)\n",
    "    print(f\"\"\"\\\n",
    "╔══════════════════════════════════════════════════════════════════╗\\n║ \\\n",
    "End of Epoch: {str(epoch).rjust(3, \" \")} | \\\n",
    "Validation Loss: {\"{:7.4f}\".format(epoch_average_valid_loss)} | \\\n",
    "Time: {str(epoch_time_minutes).rjust(6, \" \")} min. ║\\n\\\n",
    "╚══════════════════════════════════════════════════════════════════╝\\n\"\"\")\n",
    "    \n",
    "    # Save best weigths\n",
    "    best_weights, best_loss = save(model, best_weights, best_loss, loss_num)\n",
    "\n",
    "    # Early Stopping\n",
    "    if (epoch > 5):\n",
    "        if (epoch_average_valid_loss > epoch_avg_valid_losses[-1]):\n",
    "            break # Exit training loop\n",
    "    epoch_avg_valid_losses.append(epoch_average_valid_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Saved model as: Models/model1.zip\n"
    }
   ],
   "source": [
    "export_model = True\n",
    "model_path = 'Models/model%s.zip' \n",
    "\n",
    "\n",
    "def next_path(path_pattern):\n",
    "    i = 1\n",
    "    while os.path.exists(path_pattern % i):\n",
    "        i += 1\n",
    "    return path_pattern % i\n",
    "\n",
    "if export_model:\n",
    "    fname = next_path(model_path)\n",
    "    torch.save(model, fname)\n",
    "    print(f'Saved model as: {fname}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}